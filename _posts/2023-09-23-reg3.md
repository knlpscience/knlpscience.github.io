---
title: "다중 선형 회귀모형(Multiple Regression model)"
date: 2023-09-23 00:00:00 +09:00
categories: regression
published: true
math: true
---

# Chapter 3. 다중 선형 회귀 모형(Multiple Regression Model)

# 3.1 다중 선형 회귀 모형

## 3.1.1 모집단 모형

$$
E(Y)=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_{p-1}X_{p-1}
$$

$$
Y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+\cdots+\beta_{p-1}X_{i,p-1}+\epsilon_i
$$

## 3.1.2 선형 vs 비선형

$$
\frac{\partial Y}{\partial \beta_j} \text{ contains a regression coefficient if } \text{ it's nonlinear.}
$$

- 선형

  $Y=\beta_0+\beta_1X+\beta_2X^2+\epsilon$

  $Y=\beta_0+\beta_1logX_1+\beta_2logX_2+\epsilon$

- 비선형

  $Y=\beta_0+\beta_1e^{-\beta_2x}+\epsilon$

## 3.1.3 회귀 모델 행렬 표현

$\beta=(\beta_0,\beta_1,\cdots,\beta_{p-1})^T \;\;and\;\;x_i=(1,X_{i1},\cdots,X_{i,p-1})$ 이라면, 다중 선형 회귀 모형은 다음과 같이 쓰일 수 있고 $Y_i=x_i^T\beta+\epsilon_i$ 행렬 표현은 다음과 같다.

$$
y=X\beta+\epsilon
$$

- $E(\epsilon)=0$, $Cov(\epsilon)=I_n\sigma^2$
- $E(y)=X\beta$, $Cov(y)=I_n\sigma^2$

# 3.2 회귀 계수 추정

## 3.2.1 LSE

$$
L=\sum\epsilon^2_i=\epsilon^T\epsilon=(y-X\beta)^T(y-X\beta)
$$

${\delta L\over \delta \beta}=-2X^Ty+2(X^TX)\beta=0$ 의 solution은 다음과 같다($X^TX$의 역행렬이 존재한다면).

$$
\hat{\beta}=(X^TX)^{-1}X^Ty
$$

- $X^TX$의 역행렬이 존재하지 않으면 의사 역행렬(pseudo inverse)을 사용한다.
  - 의사 역행렬(pseudo inverse)정의 $X^+=(X^TX)^{-1}X^T$
    ← row가 더 긴$X$면 좌측 역행렬이라 한다.
    ← 일반적으로 정방행렬이 아닌 경우 역행렬을 가질 수 없기에 가장 가까운 역행렬로 의사역행렬을 해석할 수 있겠다.
    ← 대부분의 $X$는 row가 긴 형태이므로 의사역행렬이 default라 봐도 무방할 것 같다.

잔차 벡터$e$는 다음과 같이 정의된다.

$$
e=y-\hat{y}=y-X\hat{\beta}
$$

이 때, $\hat{y}=(\hat{Y_1},\hat{Y_2},\cdots,\hat{Y_n})=(x_1\hat{\beta}, x_2\hat{\beta},\cdots,x_n\hat{\beta})$ 이다.

special case로 단순 선형 회귀 모형을 생각해보자. 이 경우 다음을 얻는다.

$$
X^TX=\left[ \begin{matrix} n &\sum X_i \\ \sum X_i & \sum X_i^2 \\ \end{matrix}\right],\;\;\;\;X^Ty=\left[ \begin{matrix} \sum Y_i \\ \sum X_iY_i \\ \end{matrix}\right]
$$

$$
(X^TX)^{-1}={1\over n\sum X_i^2-(\sum X_i)^2}\left[ \begin{matrix} \sum X_i^2 & -\sum X_i \\ -\sum X_i & n \\ \end{matrix}\right]
$$

## 3.2.2 LSE의 성질

1. 평균과 공분산

   $$
   \begin{align*}E(\hat{\beta}) &= E\left((X^T X)^{-1} X^T y\right) \\&= (X^T X)^{-1} X^T E(y) \\&= (X^T X)^{-1} X^T X \beta \\&= \beta\end{align*}
   $$

   $$
   \begin{align*}\text{Cov}(\hat{\beta}) &= \text{Cov}((X^T X)^{-1} X^T y) \\&= (X^T X)^{-1} X^T \text{Cov}(y) X (X^T X)^{-1} \\&= \sigma^2 (X^T X)^{-1}\end{align*}
   $$

2. 가우스-마코프 정리

   다중 선형 회귀 모형 $y=X\beta+\epsilon$ 이 $E(\epsilon)=0, Cov(\epsilon)=I_n\sigma^2$ 이라면 LSE로 얻은 $\hat{\beta}$는 모든 UE중 최소분산을 갖는다(BLUE, Best Linear Unbiased Estimator).

3. MLE

   다중 선형 회귀에서도 LSE와 같은 목적식을 갖으므로 $\hat{\beta}^{MLE}=\hat{\beta}^{LSE}$

4. 유용한 결과

   - $1^T(1-H)y=1^Te=\sum e_i=\sum(Y_i-\hat{\beta_0}-\hat{\beta_1}X_{i1}-\cdots - \hat{\beta}_{p-1}X_{i,p-1})=0$
   - $\sum X_{ij}e_i=0$
   - $\sum \hat{Y}_ie_i=0$
   - 적합된 회귀 모형은 항상 점 $(\bar{X}_1, \bar{X}_2,\cdots,\bar{X}_{p-1},\bar{Y})$를 지난다.

# 3.3 GOF와 ANOVA

1. 오차제곱합 분해

   | 오차 | 자유도(df) | 기대값               | F-ratio              |
   | ---- | ---------- | -------------------- | -------------------- |
   | SSR  | $p-1$      | $MSR={SSR\over p-1}$ | $F_0={MSR\over MSE}$ |
   | SSE  | $n-p$      | $MSE={SSE\over n-p}$ |                      |
   | SST  | $n-1$      |                      |                      |

   $p: 회귀계수의 \;갯수, \;\;p-1:설명변수의\;갯수$

1. 표준오차

$$
s=\sqrt{\frac{1}{n-p}\sum(Y_i-\hat{Y}_i)^2}=\sqrt{MSE}
$$

1. 결정계수

$$
R^2={SSR\over SST}
$$

1. 회귀 모델 유의성 검정

$$
H_0:\beta_1=\beta_2=\cdots=\beta_{p-1}=0\;\;\; vs\;\;\;H_1:적어도\;하나는\; 0이\;아니다
$$

$H_0:\beta_1=\beta_2=\cdots=\beta_{p-1}=0$ 하에서 검정통계량은

$$
F_0={SSR/(p-1)\over SSE/(n-p)}={MSR\over MSE} \sim F(p-1,\;n-p)
$$

1. 다중 선형 회귀 (with 절편이 없을때)

   $$
   Y_i=\beta_1X_{i1}+\beta_2X_{i2}+\cdots+\beta_{p-1}X_{i,p-1}+\epsilon_i
   $$

   | 오차 | 자유도(df) | 기대값                 | F-ratio              |
   | ---- | ---------- | ---------------------- | -------------------- |
   | SSR  | $p-1$      | $MSR={SSR\over p-1}$   | $F_0={MSR\over MSE}$ |
   | SSE  | $n-p+1$    | $MSE={SSE\over n-p+1}$ |                      |
   | SST  | $n$        |                        |                      |

# 3.4 이차형식(Quadratic Forms)과 제곱합의 분포

1. 유용한 노테이션

   - $합 \;벡터:1=(1,1,\cdots,1)^T$
   - $J_n: 모든원소가\;1인\;n\times n \;행렬$
   - $11^T=J,\;1^T1=n,\;J1=n1,\;1^TJ=n1^T,\;J^2=nJ$

2. SST

$$
\begin{align*}SST &= \sum (Y_i - \bar{Y})^2 \\&= \sum Y_i^2 - \frac{1}{n} \left( \sum Y_i \right)^2 \\&= y^T \left( I - \frac{1}{n} J \right) y \\&= y^T T y\end{align*}
$$

1. SSE

$$
\begin{align*}SSE &= (y-\hat{y})^T(y-\hat{y}) \\&= (y-X\hat{\beta})^T(y-X\hat{\beta}) \\&= y^T y - 2 y^T X\hat{\beta} + \hat{\beta}^T X^T X \hat{\beta}\end{align*}
$$

where $\hat{\beta}=(X^TX)^{-1}X^Ty$ ,

$$
\begin{align*}
SSE &= y^T(I-X(X^TX)^{-1}X^T)y \\
&= y^T E y
\end{align*}
$$

1. SSR

$$
\begin{align*}
SSR &= y^T T y - y^T E y \\
&= y^T \left( X(X^TX)^{-1}X^T - \frac{1}{n} J \right) y \\
&= y^T R y
\end{align*}
$$

1. **보조정리**: $T,R,E$는 모두 멱등($idempotent$)행렬 이다.
2. $T,R,E$는 멱등행렬이기 때문에, $trace\;of\;matrix=Rank$ 이다.
   ← $tr(I_p)=p$ 인 점을 기억하면 쉽게 증명 가능하다.
   ← Rank가 곧 자유도와 같은건 필연인가? : 맞다! 회귀분석에서 $X$의 $Rank$는 선형독립인 독립 변수의 수를 나타낸다. 만약 $Rank$가 독립변수의 수 보다 작다면 다중공선성이 의심가는 상황이다. 다만 자유도는 고정인 점이다.

$$
tr(R)=p-1\\tr(E)=n-p\\tr(T)=n-1
$$

## 3.4.2 제곱합들의 분포

1. 제곱합들의 분포

   **유용한 정리 (I)**

   $$
   \begin{equation*}y \sim N(\mu, V) \iff y^T A y \sim \chi^2\left(\text{Rank}(AV), \frac{1}{2} \mu^T A \mu\right), \quad AV: \text{idempotent}\end{equation*}
   $$

   - 예시)

     $$
     \begin{equation*}y \sim N\left(X\beta, \sigma^2 I_n\right) \iff y^T \left(\frac{1}{\sigma^2} E \right) y \sim \chi^2\left(\text{Rank}(E), \frac{1}{2} \beta^T X^T \left(\frac{1}{\sigma^2} E\right) X\beta\right)\end{equation*}
     $$

   단, 비중심 모수 $\lambda=0$일 경우, ${SSE\over \sigma^2}\sim\chi^2(n-p)$ 가 된다.

   **유용한 정리 (II)**

   $y\sim N(\mu,V)$라고 가정하면 다음이 성립한다.

   1. $y^TAy와\;y^TBy는\;\;AVB=0 또는\;BVA=0을\;만족하면\; 독립이다.$
   2. $y^TAy와\;By는\;\;BVA=0 을\;만족하면\; 독립이다.$

   **유용한 정리 (III)**

   $E(y)=\mu,\;\;Cov(y)=V$인 랜덤 벡터$y$ 가 있으면 다음이 성립한다.

   $$
   E(y^TAy)=tr(AV)+\mu^TA\mu
   $$

   1. 제곱합들의 분포 정리

   $$
   \begin{equation*}
   \frac{SST}{\sigma^2} \sim \chi^2\left(n-1, \lambda = \frac{\beta^T X^T \left(I - \frac{1}{n} J\right) X \beta}{2\sigma^2}\right)
   \end{equation*}
   $$

   $$
   {SSE\over \sigma^2}\sim \chi^2(n-p)
   $$

   $$
   \begin{equation*}\frac{SSR}{\sigma^2} \sim \chi^2\left(p-1, \lambda = \frac{\beta^T X^T \left(I - \frac{1}{n} J\right) X \beta}{2\sigma^2}  \right)\end{equation*}
   $$

   1. 제곱합들의 기대값

      비중심 카이제곱 분포의 평균

      $$
      If\;\;W\sim\chi^2(r,\;\lambda)이면, \;E(W)=r+2\lambda \;이다.
      $$

      |       | E                        |                                                                 |
      | ----- | ------------------------ | --------------------------------------------------------------- |
      | $SSE$ | $(n-p)\sigma^2$          | $MSE={SSE\over n-p}의 \; 기댓값E(MSE)=\sigma^2이므로\; UE이다.$ |
      | $SSR$ | $(n-1)\sigma^2+2\lambda$ |                                                                 |
      | $SSE$ | $(p-1)\sigma^2+2\lambda$ |                                                                 |

# 3.5 통계적 추론(I)

$y\sim N(X\beta,\;\;I_n\sigma^2)$을 가정하자.

## 3.5.1 $\beta$ 추론

1. $\hat{\beta}$의 분포

   $$
   \hat{\beta}\sim N(\beta,\;\;(X^TX)^{-1}\sigma^2)
   $$

   ← 회귀계수가 정규분포를 따르는 근거?

   - $\hat{\beta}=(X^TX)^{-1}X^Ty$ 에서 우변을 살펴보면 $\hat{\beta}_j$는 상당히 많은 양의 덧셈으로 이루어지는 걸 확인할 수 있다. $()\cdot y_1 + \cdots+()\cdot y_p$ 의 형태로 구성되게 되는데 우리는 X를 상수취급하기로 했다. 따라서 $y$가 정규분포를 따르기에 이것의 합도 정규분포를 따른다.

   다변수 정규분포의 주변 분포 역시 정규분포이므로 다음을 얻는다.

   ![Untitled](https://machinelearningmastery.ru/img/0-821551-427318.png)

   $$
   \hat{\beta}_j \sim N\left(\beta_j, \sigma^2(X^T X)^{-1}_{j+1, j+1}\right)
   $$

   다중공선성이 문제되는 이유가 바로 $(X^TX)^{-1}$의 주대각선이 회귀계수의 분산이기 때문이다.

   - 조건수는 $X^TX$의 가장 큰 고유값과 가장 작은 고유값의 비율 즉, ${\lambda_{max}\over \lambda_{min}}$ 이다.

   - $X^TX$는 $p\times p$ 행렬인 즉, 독립변수간 유사도 행렬이라 볼 수 있고 독립변수간 유사성이 있다면 이 행렬은 full-rank가 아니게 될 것 이다. 종속인 관계가 있다는 뜻이다.

   ![Untitled](https://datatab.net/assets/tutorial/Multicollinearity.png)

2. ${\beta}_j$의 신뢰구간

   $\hat{\beta}_j$의 표준오차는 다음과 같았다.

   $$
   SE(\hat{\beta}_j)=\sigma^2\sqrt{H_{j+1,\; j+1}}
   $$

   따라서 다음을 얻는다.

   $$
   t_j={\hat{\beta}_j-\beta_j \over SE(\hat{\beta}_j)}
   $$

   그럼 신뢰구간은 다음과 같다.

   $$
   \beta_j \in \left( \hat{\beta}_j - t_{\alpha/2}(n-p)SE(\hat{\beta}_j), \;\; \hat{\beta}_j + t_{\alpha/2}(n-p)SE(\hat{\beta}_j) \right)
   $$

## 3.5.2 공동 신뢰 영역

![Untitled](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQsnNq7z_alVLn5JczIP1kwa-zdCa2MWd5Tv12TV0a7JxzQ5G8ie-3O7JD6k2GDwAECBhE&usqp=CAU)

1. $\beta$의 공동 신뢰 영역

   아래와 같은 2차 형식을 고려해보자.

   $$
   Q=(\hat{\beta}-\beta)^TX^TX(\hat{\beta}-\beta)
   $$

   $(\hat{\beta}-\beta)$는 다음 분포를 따른다.

   $$
   (\hat{\beta}-\beta) \sim N(0,\;\;\sigma^2(X^TX)^{-1})
   $$

   모수 $\beta$는 상수이므로 분산이 0이어서 영향이 없고 평균에만 영향을 끼친다.

   이차 형식 정리에 따라 다음을 얻는다($Rank(Q)=p$).

   $$
   {Q\over \sigma^2}\sim \chi^2(p)
   $$

   ← 다중공선성이 있을 경우 $p$가 안되는가?

   2차 형식에서의 독립 조건을 만족하므로 $Q$와 $SSE$는 독립임을 알 수 있다.

   따라서, 다음이 성립한다.

   $$
   \frac{\frac{Q}{\sigma^2}/p}{\frac{SSE}{\sigma^2}/(n-p)} = \frac{1}{ps^2} (\hat{\beta} - \beta)^T X^T X (\hat{\beta} - \beta) \sim F(p, n-p)
   $$

   마침내 다음과 같은 등식을 얻을 수 있다.

   $$
   \Pr\left[\frac{(\hat{\beta} - \beta)^T X^T X (\hat{\beta} - \beta)}{ps^2} \le F_\alpha(p, n-p)\right] = 1 - \alpha
   $$

   그리고 다음 조건을 만족하는 ‘$\beta$ 의 집합’을 공동 신뢰 영역으로 부른다.

   $$
   \{\beta : (\hat{\beta} - \beta)^T X^T X (\hat{\beta} - \beta) \le ps^2 F_\alpha(p, n-p)\}
   $$

2. $\beta$의 검정

   | 귀무가설            | 대립가설                | 기각역                                                                           |
   | ------------------- | ----------------------- | -------------------------------------------------------------------------------- |
   | $H_0:\beta=\beta_0$ | $H_1:\beta\neq \beta_0$ | $ (\hat{\beta} - \beta)^T X^T X (\hat{\beta} - \beta) > ps^2 F\_\alpha(p, n-p) $ |

## 3.5.3 동시신뢰구간(simultaneous C.I.)

1. **본페르니**(Bonferroni Method) : 매우 보수적

   “$E_i:신뢰구간이\;\;모수\theta_i를\;\;포함하는 \;\;사건$” 이라고 하면, $1-\alpha^*$하에서

   $$
   Pr[E_i]\ge 1-\alpha^*
   $$

   이제 “$Q:모든\;\;신뢰구간이\;\;전부\;\;정답인\;\;사건$”이라 하자. 그러면 다음과 같이 쓸 수 있다.

   $$
   Pr[Q]=Pr[\cap E_i]
   $$

   여기서 만약 본페르니 부등식을 적용한다면(즉, $Pr[\cup E_i]\;\;\le\;\;\sum Pr[E_i]$)

   $$
   Pr[Q]=1-Pr[\cup E_i^c]\;\;\ge\;\; 1-\sum P[E_i^c]\;\;\ge\;\;1-g\cdot\alpha^*
   $$

   따라서, 각각의 신뢰수준은 $1-{\alpha\over g}$ 가 되는게 합리적이다($1-\alpha$ 신뢰수준 하에서 검정을 원한다면).

   결과적으로, $\beta_j$ 에 대한 본페르니 신뢰구간은 다음과 같다.

   $$
   \hat{\beta}_j \;\;\pm\;\;t_{\alpha/2g}(n-p)\cdot SE(\hat{\beta}_j)
   $$

   하지만, $g$(검정해야할 갯수)가 클수록 $t_{\alpha/2g}$도 엄청 커지기 떄문에 매우 보수적이라는 거다.

2. **쉐페**(Scheffe Method) : 보수적

   $$
   \hat{\beta}_j\;\;\pm\;\; \sqrt{g\cdot F_\alpha(g,\;\;n-p)}\times SE(\hat{\beta}_j)
   $$

3. **최대 모듈러스**(Maximum Modulus Method) : 덜 보수적

   $$
   \hat{\beta}_j\;\;\pm \;\;u_\alpha(g,\;\;n-p)\times SE(\hat{\beta}_j)
   $$

   - $\hat{\beta}_j$ : $j$번째 회귀계수의 추정치

   - $u_\alpha(g,\;\;n-p)$ : 동시신뢰수준 *α*와 자유도 *n*−*p*에 대한 최대 모듈러스 통계량

   - $SE(\hat{\beta}_j)$ : $j$번째 회귀계수의 표준오차

   - _g_: 검정하고자 하는 회귀계수의 수

   - _n_: 관측치 수

   - _p_: 모델에 포함된 독립변수의 수

     이 수식을 사용하여 각 회귀계수에 대한 동시신뢰구간을 구할 수 있습니다. 최대 모듈러스 방법은 여러 신뢰구간 중 가장 넓은 신뢰구간을 기준으로 하기 때문에, 이 방법으로 구한 동시신뢰구간은 각각의 신뢰구간보다 넓을 수 있습니다.

     [→ 최대 모듈러스 설명](https://www.notion.so/1e69f6baa713479bbcb9d52b590882df?pvs=21)

## 3.5.4 평균 반응 $E(Y)$에 대한 추론

1. $E[Y(x_0)]=x^T_0\beta$ 추론

   $x_0=(1,\;x_{01},\;x_{02},\;\cdots,\;x_{0,\;p-1})^T$ 이 주어지고, $E[Y(x_0)]=x^T_0\beta$ 을 $x_0$에서의 평균 반응 이라 하면 점 추정치로 다음을 얻을 수 있다.

   $$
   \hat{Y}{(x_0)}=x^T_0\hat{\beta}
   $$

   그리고 이것의 평균은

   $$
   E[\hat{Y}{(x_0)}]=x^T_0{\beta}
   $$

   이것의 분산은

   $$
   Var(x_0^T\hat{\beta})=x_0^TCov(\hat{\beta})x_0=\sigma^2\cdot x^T_0(X^TX)^{-1}x_0
   $$

   따라서, 신뢰구간은

   $$
   x^T_0\hat{\beta}\;\pm\;t_{\alpha/2}(n-p)\cdot s\sqrt{x^T_0(X^TX)^{-1}x_0}
   $$

   이 결과를 어느 회귀계수의 선형결합의 신뢰구간으로 적용할 수도 있다.

   $$
   q^T\beta=q_0\beta_0+q_1\beta_1+\cdots+q_{p-1}\beta_{p-1}
   $$

   $$
   q^T\hat{\beta}\;\pm \;t_{\alpha/2}(n-p)\cdot s\sqrt{q^T(X^TX)^{-1}q}
   $$

   예를들어, p=5$p=5$$\beta_2-\beta_3$의 신뢰구간을 원할 떄 $q=(0,0,1,-1,0)^T$으로 두고 구할 수 있는 것이다.

2. $x_0$에서 새로운 반응 추론

   새로운 반응 $Y(x_0)$의 신뢰구간은 다음과 같다. ← 이전과의 차이점은 평균반응이 아니라는 것!

   $$
   x^T_0\hat{\beta}\;\pm \; t_{\alpha/2}(n-p)\cdot s\sqrt{1+x^T_0(X^TX)^{-1}x_0}
   $$

## 3.6 통계적 추론(II)

### 3.6.1 부분 F 검정(Partioal F-test)

1. 귀무가설

$$
H_0:\beta_r=\beta_{r+1}=\cdots=\beta_{p-1}=0
$$

1. 완전 모형(Full model) 과 축소 모형(Reduced model)

   1. 완전 모형(Full model)

   $$
   Y_i=\beta_0+\beta_1X_{i1}+\beta_1X_{i2}+\cdots+\beta_{p-1}X_{i,p-1}+\epsilon_i
   $$

   $$
   SST=SSE(F)+SSR(F)
   $$

   1. 축소 모형(Reduced model)

      귀무가설 하에서

      $$
      Y_i=\beta_0+\beta_1X_{i1}+\beta_1X_{i2}+\cdots+\beta_{r-1}X_{i,r-1}+\epsilon_i
      $$

   $$
   SST=SSE(R)+SSR(R)
   $$

   1. 검정 통계량

      1. 모티베이션

      $$
      Reject\;\;H_0\;\;if\;\;SSR(F)-SSR(R)\;\; is\;\;large
      $$

      즉, 설명가능한 분산의 변화량을 조사하겠다는 뜻이다.

1. 유도

   $y=X\beta+\epsilon$ 을 분해하면

   $$
   x=[X_1\vdots X_2],\;\;\;\;\beta= [\beta_1\vdots \beta_2]^T
   $$

   $$
   y=X_1\beta_1+X_2\beta_2+\epsilon
   $$

   그러면 귀무가설은 $H_0:\beta_2=0$ 로 쓸 수 있다. ← 2분할 후에 두번째 애들의 설명력을 보려는 것

   그리고 다음을 얻을 수 있다.

   $$
   SSR(F)=y^T(X(X^TX)^{-1}X^T-{1\over n}J)y
   $$

   $$
   SSR(R)=y^T(X_1(X_1^TX_1)^{-1}X_1^T-{1\over n}J)y
   $$

   그러면, 다음을 얻는다(extra sum of squares).

   $$
   SSR(F)-SSR(R)=y^T[X(X^TX)^{-1}X^T-X_1(X^T_1X_1)^{-1}X^T_1]y
   $$

   귀무가설 $H_0:\beta_2=0$ 하에서, ${SSR(F)-SSR(R)\over \sigma^2}\sim\chi^2(p-r)$을 따르기 때문에 검정 통계량은

   $$
   F_0={(SSR(F)-SSR(R)) / (p-r)\over SSE(F) / (n-p)}={SSE(R)-SSE(F) / (df_R-df_F)\over SSE(F)/df_F}\sim F((p-r),\;\;(n-p))
   $$

   이고 $F>F_\alpha(p-r,\;\;n-p)$이면 귀무가설을 기각한다. 이것을 Partial F-test라 한다.

## 3.6.2 일반화 선형 검정

1. 귀무가설

   일반화된 선형 테스트는 다음과 같이 쓴다.

   $$
   H_0:C\beta=m,\;\;\; H_1:C\beta \neq m
   $$

   $C$는 $Rank(C)=q$인 $q\times p$ 행렬이다. 그리고 $m$은 $q\times 1$인 벡터이다.

   예를 들어, $H_0:\beta_1-\beta_2=0,\;\;\beta_3+2\beta_4=3$ 검정을 원한다면,

   $$
   C=\left[
   \begin{matrix}
       0 & 1 & -1 & 0 & 0 \\
       0 & 0 & 0 & 1 & 2 \\
   \end{matrix}
   \right],\;\; m=\left[
   \begin{matrix}
       0 \\
       3 \\
   \end{matrix}
   \right]
   $$

   또한, $\beta_1=\beta_2=\cdots=\beta_{p-1}$ 을 검정하고싶다면 다음과 같이 쪼갤수 있다.

   $\beta_1-\beta_2=0,\;\; \beta_2-\beta_3=0,\;\; \cdots,\beta_{p-2}-\beta_{p-1}=0$ 이 경우 $C_{p-2 \times p}$는

   $$
   C=\left[
   \begin{matrix}
       0 & 1 & -1 &0& \cdots & 0 \\
       0 & 0 & 1 &-1& \cdots & 0 \\
   \vdots & \vdots &\vdots &\vdots &\ddots &\vdots\\
   \\ 0&0&0&0&\cdots &-1
   \end{matrix}
   \right],\;\; m=\left[
   \begin{matrix}
       0 \\
       0 \\
   \\
   \vdots\\
   \\0
   \end{matrix}
   \right]
   $$

1. 검정 통계량

   $$
   F_0={SSE(R)-SSE(F)\over q \cdot MSE(F)}\sim F(q,\;n-p)
   $$

   이제 우리는 다음을 알 수 있다.

   $$
   SSE(R)-SSE(F)=(C\hat{\beta}-m)^T[C(X^TX)^{-1}C^T]^{-1}(C\hat{\beta}-m)
   $$

   그래서, 검정통계량은 다음과 같이 쓸 수 있다(derivated by, 라그랑주 승수법).

   $$
   F_0={(C\hat{\beta}-m)^T[C(X^TX)^{-1}C^T]^{-1}(C\hat{\beta}-m)\over q \cdot MSE(F)}\sim F(q,\;n-p)
   $$

   [증명](https://www.notion.so/fe5dfc36fafc4d40a4e3465e3487ca6a?pvs=21)

## 3.6.3 더 많은 공변량 포함(Inclusion of More Covariates)

1. 증강 모형 (Augmented Model) ← 이전에는 [완전 vs 축소] 였다면 이번에는 [증강 vs 완전]

   $E(y)=X\beta$를 고려해보자. 증강 모형은 다음과 같다.

   $$
   E(y)=X\beta+Z\gamma=\left[
   \begin{matrix}
       X & Z
   \end{matrix}
   \right] \left[
   \begin{matrix}
       \beta \\ \gamma
   \end{matrix}
   \right] \equiv W\delta
   $$

   $Rank(X)=p$ 인 $X_{n\times p}$ , $Rank(Z)=k$ 인 $Z_{n \times k}$ 이라면 $Rank(W)=p+k$ 인 $W_{n \times(p+k)}$

2. LSE of $\beta$ and $\gamma$

   LSE of $\beta$ and $\gamma$ 를 얻기위해 다음을 최소화 해야한다.

   $$
   \begin{align*}\epsilon^T \epsilon & = (y - X \beta - Z \gamma)^T (y - X \beta - Z \gamma) \\& = y^T y - 2 \beta^T X^T y - 2 \gamma^T Z^T y + 2 \beta^T X^T Z \gamma + \beta^T X^T X \beta + \gamma^T Z^T Z \gamma\end{align*}
   $$

   $\beta\;and\;\gamma$에 대해 미분을 하면 다음을 얻는다.

   $$
   -2X^Ty+2X^TZ\hat{\gamma}_G+2X^TX\hat{\beta}_G=0
   $$

   $$
   -2Z^Ty+2Z^TX\hat{\beta}_G+2Z^TZ\hat{\gamma}_G=0
   $$

   위 식을 풀면 다음을 얻는다.

   $$
   \begin{equation*}\hat{\beta}_G = (X^T X)^{-1} X^T (y - Z \hat{\gamma}_G) \tag{*}\end{equation*}
   $$

   $$
   \begin{equation*}Z^T Z \hat{\gamma}_G = Z^T y - Z^T X (X^T X)^{-1} X^T (y - Z \hat{\gamma}_G) \tag{**}\end{equation*}
   $$

   정리하면, 다음을 얻는다.

   $$
   \hat{\gamma}_G = (Z^T R Z)^{-1} Z^T R y
   $$

   $$
   R = I - X (X^T X)^{-1} X^T
   $$

3. $H_0:\gamma=0$ 검정

첫번째, $SSE(R)=y^TRy$ 임을 참고하라 그리고 영향도 행렬을 Full model하에서 $H_G=W(W^TW)^{-1}W^T$ 이라하고 $R_G=I-W(W^TW)^{-1}W^T$이라 하자. 그러면,

$$
\begin{align*}\text{SSE}(F) & = y^T R_G y \\& = (y - W\hat{\delta}_G)^T(y - W\hat{\delta}_G) \\& = (y - X\hat{\beta}_G - Z\hat{\gamma}_G)^T(y - X\hat{\beta}_G - Z\hat{\gamma}_G)\end{align*}
$$

한편,

$$
\begin{align*}y - X\hat{\beta}_G - Z\hat{\gamma}_G & = y - X(X^TX)^{-1}X^T(y-Z\hat{\gamma}_G) - Z\hat{\gamma}_G \\& = [I - X(X^TX)^{-1}X^T](y-Z\hat{\gamma}_G) \\& = R(y-Z\hat{\gamma}_G)\end{align*}
$$

이기 때문에 다음을 얻는다.

$$
\begin{align*}
\text{SSE}(F) & = y^T R_G y \\
& = (y - Z\hat{\gamma}_G)^T(y - Z\hat{\gamma}_G) \\
& = y^T R y - 2\hat{\gamma}_G^T Z^T R y + \hat{\gamma}_G^T Z^T R Z \hat{\gamma}_G \\
& = y^T R y - \hat{\gamma}_G^T(Z^T R y - Z^T R Z \hat{\gamma}_G) \\
& = y^T R y - \hat{\gamma}_G^T Z^T R y \\
& = \text{SSE}(R) - \hat{\gamma}_G^T Z^T R y
\end{align*}
$$

따라서, F 검정 통계량은 다음과 같다.

$$
\begin{align*}F &={SSE(R)-SSE(F) / (df_R-df_F)\over SSE(F)/df_F}\\&={\hat{\gamma}_GZ^TRy / (n-p-k)-(n-p)\over(y^TRy-\hat{\gamma}_GZ^TRy)/(n-p-k)}\\&={\hat{\gamma}_GZ^TRy /k\over(y^TRy-\hat{\gamma}_GZ^TRy)/(n-p-k)}
\end{align*}
$$

$H_0:\gamma=0$ 하에서 $F(k,\;\;n-p-k)$를 따른다.

## 3.7 적합결여검정(Lack of Fit Test)

1. 귀무가설 (회귀분석에서 중요한 가정 중 하나는 현재 설정된 회귀식이 옳다는 것이다)

   $$
   H_0:E[Y_i]=X^T_i\beta
   $$

   만약 이 과정이 옳지않다면 많은 문제가 발생한다.

   $s^2$이 더이상 $\sigma^2$의 $UE$가 되지 못한다(일반적으로 과대추정← 전체가 아닌 일부를 기반으로 하기때문에 왜곡되거나, 두꺼운 꼬리를 갖거나, 유사하게 이분산 일 경우 과대평가 하게됨)

   1. $H_0$가 맞다면

      $E(SSE)=\sigma^2(n-p)$

   2. $H_0$이 틀리다면

      $E(SSE)=\sigma^2(n-p)+\sum^m_{i=1}(E(Y_i)-X^T_i\beta)^2$

   ### 조사방법

   1. 잔차분석
   2. $s^2$이 $\sigma^2$을 과대추정하는지 조사해본다.

2. 검정 통계량

   $Y_{ij}$을 $x_i$에서 $j$번째 반복이라고 가정하자. 그러면, 다음과같이 분리 시킬 수 있다.

   $$
   (Y_{ij}-\hat{Y}_i)=(Y_{ij}-\bar{Y}_i)+(\bar{Y}_{i}-\hat{Y}_i)
   $$

   위로 부터 다음을 얻는다.

   $$
   \sum^m_{i=1}\sum^{n_i}_{j=1}(Y_{ij}-\hat{Y}_i)^2=\sum^m_{i=1}\sum^{n_i}_{j=1}(Y_{ij}-\bar{Y}_i)^2+\sum^m_{i=1}n_i(\bar{Y}_{i}-\hat{Y}_i)^2
   $$

   $\sum^{n_i}_{j=1}(Y_{ij}-\bar{Y}_i)=0$이기 때문에 그렇다. 즉, $SSE$를 분해하면 다음과 같다.

   $$
   SSE_{n-p}=SSPE^{순오차제곱합}_{n-m}+SSLF^{적합결여제곱합}_{p-m}
   $$

   $SSPE:$ 설명할 수 없는 오차

   $SSLF:$ 모델적합에 실패해 생긴 오차

   $$
   MS_{PE}={\sum^m_{i=1}\sum^{n_i}_{j=1}(Y_{ij}-\bar{Y}_i)^2 \over \sum^m_{i=1}(n_i-1)}=\sigma^2(n-m)\\ \ \\ \therefore E(MS_{PE})=\sigma^2
   $$

   $$
   MS_{LF}={\sum^m_{i=1}\sum^{n_i}_{j=1}(Y_{ij}-\bar{Y}_i)^2 \over \sum^m_{i=1}(n_i-1)}={SSE-SSPE\over m-p}\\ \ \\ \therefore E(MS_{LF})=\sigma^2 + {\sum n_i(E(Y_i)-X^T_i\beta)^2\over m-p}
   $$

   $H_0$이 맞다면 ${\sum n_i(E(Y_i)-X^T_i\beta)^2\over m-p}=0$이다. 즉, ${E(MS_{PE})\over E(MS_{LF})}$의 비율을 구해서 비교하자는 의미다.

   $$
   F_0={MS_{LF}\over MS_{PE}}\sim F_\alpha(m-p,\;\;n-m)
   $$

   ← $MS_{LF}와 MS_{PE}$는 독립이다.

   그러면 ANOVA table은 다음과 같이 주어진다.

   | source      | $SS$   | $df$  | $MS$      | $F-ratio$                    |
   | ----------- | ------ | ----- | --------- | ---------------------------- |
   | Regression  | $SSR$  | $p-1$ | $MSR$     |                              |
   | Error       | $SSE$  | $n-p$ | $MSE$     |                              |
   | Lack-of-Fit | $SSLF$ | $m-p$ | $MS_{LF}$ | $F_0={MS_{LF}\over MS_{PE}}$ |
   | Pure Error  | $SSPE$ | $n-m$ | $MS_{PE}$ |                              |
   | Total       | $SST$  | $n-1$ |           |                              |

   $F_0={MS_{LF}\over MS_{PE}} \approx1$이면, 선형 또는 타당한 모형이다.

   $F_0={MS_{LF}\over MS_{PE}} \gg1$ 이면, 비선형 또는 타당하지 못한 모형 으로 판단한다.

   ## 3.8 그 외

   ### 3.8.1 회귀계수 표준화(Standardization of Regression Coefficients)

   Let $j$번째 설명변수에서의 편차제곱합

   $$
   S_{X_j}=\sum_{i=1}^n(X_{ij}-\bar{X}_j)^2
   $$

   $y$의 편차제곱합

   $$
   S_{Y}=\sum_{i=1}^n(Y_{i}-\bar{Y})^2
   $$

   and let $X_j$의 표준화된 회귀 계수

   $$
   \hat{\beta}^*_j = \sqrt{\frac{S_{X_j}}{S_Y}} \hat{\beta}_j, \quad j = 1, 2, \dots, p-1
   $$

   Futher, let

   $$
   Y_i^*={Y_i-\bar{Y}\over \sqrt{S_Y}}={Y_i-\bar{Y}\over \sum_{i=1}^n(Y_{i}-\bar{Y})^2}
   $$

   $$
   W_{ij}={X_{ij}-\bar{X}_j\over \sqrt{S_{X_j}}}={X_{ij}-\bar{X}_j\over \sum_{i=1}^n(X_{ij}-\bar{X}_j)^2}
   $$

   그러면, 스케일링된 정규방정식은 다음과 같다.

   $$
   (W^TW)\hat{\beta}^*=W^Ty
   $$

   참고로 공분산이 스케일링된 즉, 상관계수 행렬$W^TW$은 다음과 같다.

   $$
   W^TW=\left[
   \begin{matrix}
       1 & r_{12} & r_{13} & \cdots & r_{1,\;p-1} \\
       r_{12} & 1 & r_{23} & \cdots & r_{2,\;p-1} \\
   \vdots & \vdots  &\vdots &\ddots &\vdots\\
   \\ r_{1,\;p-1} & r_{2,\;p-1} & r_{3,\;p-1} & \cdots & 1
   \end{matrix}
   \right]
   $$

   그리고 $W^TW$의 $j$행 $l$열 의 원소는 다음으로 표현할 수 있다.

   $$
   \{W^TW\}_{jl}=\sum^n_{i=1}W_{ij}W_{il}={\sum^n_{i=1}(X_{ij}-\bar{X}_j)(X_{il}-\bar{X}_l) \over \sqrt{S_{X_j}S_{X_l}}}=r_{jl}
   $$

   또한,

   $$
   W^Ty^*=\left[
   \begin{matrix}
   r_{_{Y1}}\\
   r_{_{Y2}}\\
   \vdots \\
   r_{_{Y,\;p-1}}
   \end{matrix}
   \right]
   $$

   ## 3.8.2 다중공선성(Multicollinearity)

   두 개의 공변량(독립변수)를 가정하고 표준화된 공분산을 사용하면

   $$
   W^TW=\left[
   \begin{matrix}
   1&r_{_{12}}\\
   r_{_{12}}&1
   \end{matrix}
   \right]
   $$

   이것의 행렬식은 $ \vert W^T W \vert=1-r^2\_{12}$ 이고 역행렬은 다음과 같다.

   $$
   (W^TW)^{-1}={1\over 1-r^2_{_{12}}}\left[
   \begin{matrix}
   1&-r_{_{12}}\\
   -r_{_{12}}&1
   \end{matrix}
   \right]
   $$

   : 두 설명변수간 상관이 높으면 분모가 $\rightarrow 0$ 으로 수렴하여 전체는 $\rightarrow \infty$ 이 되므로 $Cov(\hat{\beta}^*)$의 값이 매우 커져 추정의 정확도가 낮아진다.

   ## 3.8.3 LSE의 기하학적 관점

   $$
   \Vert y \Vert^2=\Vert \hat{y} \Vert^2+ \Vert e \Vert^2
   $$

   ![Untitled](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARMAAAC3CAMAAAAGjUrGAAAA/FBMVEX////M/6rO/6wAAADR/67S/6/U/7HV/7LB8aHH+aaVunzF9qTK/6a555p3lWOQtHhOYUFwjF204ZZHWTus1499nGiiyoeEpW5UaUa+7Z613ZrX1NlddE2cw4Kn0Yuv2pJlflS7u7vi3uTx7vOmuJuJq3Jsh1qq2opBUTY1QixZb0qurq7r6O349fqNn4K88Jmau4Sdyn/S0NS/v7+YoZKw4Y8pMyIcIxc3RS6ip58fJxoYGBhpaWmzua/Jxcunt52cuIqhuJMRFQ6Npn2Rn4is0JWMqHmDg4NFRUWgoKBwcHBgYGAzMzMmJiZOUkyZt4cNABRukVd3gnCgw4gvOy0nAAAQyElEQVR4nO2dC3vauLaGI2lJvsUYfIUpBtJsCJDekkxJJp22M2k7M3tmz97nnP3//8uRbAMGG4cAlp003/MkoQUeSy9LS7dP5uioIv3589EReV3V1eupyw9HRz9XXYia6ZQc/f4cJmsiJydVF6F2+uO55WT0V9UFqJ8uz6suQd304sPvVRehpvrni6pLUDv9Tp7bz5r+JH88M1nVX4T8/MxkRf8mhPzzsupS1Ep/cCTkZdWlqJX+JZCQ8+e2s9DrvyMk5PJ5xjPX6YcYyYdnJnO9JIn+emaS0qtzweTPqotRL53/cSK64ue+OCVydfSCnF9dVV2OGun8b/7r5IQzOf10WnVhaiISBcjr86vXv709e/ePj89g4jDhOn31BRDGGB271vcOhiR55JQjiSXAvL3gEXP5nYKZh8nRv3sYpbQA8+n72+SYh8kXy19hshYx3xOY8w/x3zcQ5DBJwCAO5j+fvxswSZi8QaD3NzCZgzkWYE5ePHkwSZh8TcLhHs0j5mmDicPk6zFC4PXuh7KIGP/pgonDRCDhTNztmCzBhE8STBQmPwokCBoPYbJoSu/DXz5fPSUwJ+RojgRB68FMFhEjwJw8kfVcESYJEl65XYikI0bnEfPysUfMCc8mP72dV0vvZOsKDBDQLWnNI+bqMUcMD5Of3s4rDC07W3ePBKwxfkCjwjGYXz8+0qbEs8kSCWdiZquuzHRw4OG5l4M503/9ePWq6jo+VOTq4xIJgpGZrR1YN111t0TzKMGc/H2VQoJwz8irGQlhj/S7APPpcYD58NfblbrmhAkC/65FEfTznnuCYE7IaqOAQbbfwWbXJoD9sQWZ53YGU+N1qv/R6UqhYaRm6oEcYHc+Rb39maTAWHUFc05WkSA6WGeCe5MAzPGsD/6BmCzAxAt4NQNzSfS1akIjEyfJE/iwTBZgji+sOi35Xv7vepjwsUh+78LcoWZ5G3jtD6Y2a+Gf3t9mPnnItJ2o2GqXIDcMt1xb2QdMtWvhl2chyTQG2s1jYgxIQDHeYg1uTzCo2rXwywvlLpsg6DRnzIaN6ahcGumLVbcW/uKC5YQJos0sE6w5SJOBI3XJKtbCX1wAvc7rR7JNRyCpRJLXwl+GAHlhgrC+HhF8yKaVnEeKFEfM/0lBglhONsltO5UiiQR6+VMkgSQ/TBBtr8dJ0ygbCb5naQbC8lemXonBK7se5jJprjIBZ8eFk+2F3SBawgOav8IpBYloMxvCBIG1Wt7ykfCPZ9ASZfFuLBZMMyucMpCcRmmE3uaGCR+erZSn2SkPCYigEL/oqCHKohBfyeYuuCj/QNFpxAL7+WGC6Dj9jzKRoFZjatJha+AqMRPw2qPMArkUJL9FLJT8bCIwpB63c9arDyXaHCqmb7UVRIyYCTKItT4lhYvyramvYyQ8TDbUNsWETe1Drw2kJRb46FgH5litiAm2yfoHBWfSkCA22xAmPDbmsGi3VCSI8A6OTUKg02HEBBtNn7CVl8DZp/KRfIkrvDGbcCnJXzZwd0MC270NiM+AdQOmXLutBuWDFEcD0k+/WQaSowQJD5NgU8FhED+j7IgEUBCgbd4JPmk1fPXWbzj2zdjGnbYOmtNMTbewDCRvEiSbs4nIJ1GaY4PebkiG17qVO7fMvhT1TcC4b/LOmHfKts252PZyuwS/l+ByfzO/mrI5TBImbOQve4AtG0Ok4ZACQGBt1V/hxa/oAV4xjuH3Eg7NLJAUhQmiDofBWuECCWiep91LBUAMv3hS6Ikm9+Ct5aykIPm6uFxBNhEdIkassUSCh7dheDu8p5Jmo9XTGg1V5fmynVn23kH4rYRjiV+PF5frFYQJDyLMvOU+GLYsxofflgWFq7HgEooczJpD1iHKxpfVFUlxmPBRlBKkRpPUDnuizmooFG2biglRZiVSufObBgZis3DG1p/cAclP5SP5cYmEh0lRcWjTGqaQjJxOtKDSsXQu0SlozoAa0bYQUKH4hTDkQwvcIVrY3n+KJBsJ73S8ogbPJ0Kpz5mPveM9MKwIRTM1/UYxxbgTAqfZbI7jkKE6MXi7u0beZO/VbOlI7gsTa5BOBzRotsUoCrui/o6w4uM+UaYRp1ScYHvU9gC8JlX43z2ZyECy8O8JsUlRmFDdSyMB60ZhcZxE9Y8eamS4bsnHaMz4XIW2G6A0i7LVNjr+UQaSFRtSUZhQvaFMUshgeGv0189p0LtGhuqYz3GJr9zpzCZ5Pqc6I0FKUZjQsMFgnHoeQzAYRlu5YoSJ4x/m0MXjePAJut6nfqirxAsa6n4DNhlIzleQYLcgTKjfYijNRP32ww8/8J9v8x/+z3c/tN4l/73+1Lf/kvjvHlCOv8pGUphNoDfiPU6KifoN0DIY4rgAf2ZRvBYn8UME4ZitzlkeA5KiMAF3EHXCizeouZ83pgXzgr1H9W/KR3LyfrVa7CabHhOB242RtJI65yMpVxUgKQgT6MdIFm3HeJpIrtaQIGVjmIA9nQ/SYyYVIMEykJytj6z6m8IEzMUEH27EI60CJF/KN1R8OltPhmy8IUyw2V68FgvLmvatzNrnl6ESJBvDBHeaqdeKcXsFSH4rH8llBsnGMFlFAmOlAiQgA8lFUs3lAIp3OrmzeGw46dwB7bACJMPynbEvEiSY8hncnI6+bpmO/3sVCYLmtSc7v8pEAh2n7V3PdyXwBiRr5Wsah1hjfojAkuPMiuobtk1GUashriusHlkPkLA0rg9iZGBISyYSfjVF02i0ZaU1bnTa6K67O3KQoKbkMJHl35trPLlOpi9AbMjWP8/SeJDtme0lzb+XXK7VUlBy8IKORt3sgrqTtTRqU6lMpPn3ErHrjqK34ySCTeJnBnHNHP+eIbXtyPPvzUXs7p3BoiQCPXF6CdL1BadMs9pWkuffS4T7Ezu41VvXTIxVu/qEgTdLuQXy/XvadpaAg0iif29+RWvEWNgUHQnGfGBGOuAuEyhs8O+p8vKJRP/eXHQQKMp0qHAm5tgHY9I1lEWy2GBpBOQOZDGR4t/7sppE2Q2Z3jSBNikWHiDTtjt0zoROzeyoFlPVa7jdjB2TFizB7oNEnn9vKYLcDkWKo8x3aLDSjEeprNvPVJPPjVpDjRprW1yYWbOZxQ6eZCpBgu1bURPDC7y5R6zjdcXJT6x0+5kGgu1RGJm0Vo+hY3fCpwaNyW43z9ks/F6if2+pTi/Z6V6ejhUnHzBzJ67mRpGTejVYWuzbjfIJTrxYCNlIoZhq9qGRyPTvpS8c/zbT7gdQemPCW5ABuob7eIkFrMiFycNFbHT1g6GtBgF/I+212y7e645CeUhk+veyYs1lT4JpOCOERLYAXs8+NnvYiOElTKjZECmHDmcKcgQd9850yf7OI/lIvhZc3yQLGy52o1uT38yrGCVeH2Edz5lQfXbXEog0okbnu2g7ZMaBmcg2q2XE2qkBBzYGnEm40udEMWP3qWcIg4Bg1mbCbH8XdU2MqOYBbvFRKyQ8TNJ+EGXgkOu8QRmGoYEsZRYFkioaTzyhNohPyEHnRdKdWRmthAlSuoGy4XyXaDuQMOlgbLeuI6NWb6wo1viAQ9vqkYhssvyQlYHHeJ+b+0qwDGZ2JwLJhCLNUTxHOC+CBqN6Zmj7mJGshgnjSNCmXpXqoTM1LZ5vblRMmxo2CM+xtHvb98kBT3tJ9+9lhDupMGGjxsb+A3iH0zIUrFFkUKx5ng2W52niJOV0lDM12lXyzWoZsWl3gYG1WpuQUBTMAkzx4u4m8ShWoCB8Ani4KKkBknSYsMYgHwmmRmuiRwAyd6LG9kFXU+T797JKhQlHkrtxg5k5HfdYsiGUffqgSOSb1bIV6ixGFszr5kUJ0B5PrPFCgKmXvOhYBySIdecgWDDNiRKeWCctg85PW5W9DluBWS2jZZiwoJ1FkiTW5LVWzl1jD6s6IOFhMo3DhA3b6w2HJ9bRjb446Cdhqb4K/15GizBhVnMNiUisjrtcZcWbvvrgYMJfJCBZ9+9lRZMwoZazdrg7lViTEpd327XkApWY1bLFUOMwofrqDC6dWBNpZbedmiCZZxMa3qSRRIkVrQ069j1wc5+k+Pcu7q9EEiY0nCxfi6k6ugnXTlCLcwUHp7Aiqf69ItFBtFrmz5aJVCTW3vr2FXbDkhuOTLNaoXiY8Al+CglPrOPVxBq/bq+TJdtICpKMjSRPUZjQ3iypMEAmsUbC/p4H1e6VXLNaoQweJrR3G0MQiXW4nlhjHXgrK6MaIRFhAu51NIuJEmv+LnjZQSLHv5dr+82KZxPavxPZgpltPmLNzxk4KDuXyPXvFYoOmopAIhJrN5tYE5XfCUv27xXKIB37TqMbEmsiteyhGviy/XsFoqO2SnjL2JRYhcpfL5Hu3yuUQfxbszXZkFgjYbf0Hke6f69IdDS7GzfdYmuRWnKfU4F/r0gdQhxVYUVbEKXcpj6tKvx7RTLGDeduMtJNzFb2ZlKTwQ37owdTNf69IlFKQfUTMGyxaTWaU4HS02vtkMTigxOm9YLp9ZiDQTxilCbRoxyzOGleluSY1Xb9XCMw/eFgPBlYZpeQscnKH6pV5t97SBmBUmzrrchaMjJ2u2fhAy5XsX/vASUFFDEhdy1f3XVfvKN2kNop7sXl+PcKDSZbC8bEaVmugdWw0R14PW0HMEPismmrcHescrPa9sKgGfMeiLcl6ISt7iDoafAgMOzat/O34hfXeTRIxA2P1k7ScjDY1AUYd/uIgWDcLsxGNXBmbSnau/aCuzBrsedgUARm2McU7geDxfKd8J1vev7RIMHWCPFIaAQo78tUeQ0p2NagO7JsdA8Y1pp4gM2G99iRCI+9acbHjJFHhsxzMocsRHeN+sPRgIMRKSYfDLWsHqEwwf1BbqQcS/HvHWYMjifOONkgVW5DmnPMOAEDAsyg2+JTpZwUA/0BU4gLAGYukxqY1bYWBANFSY4Zw3A8KHR2iojR3CAHDNZnIXWdpk0747x3PiYkiI5dxR+zpEEQ/X6Lmuit+VyJgwk7y04Ji9v38X+aTt7dIGQ4sw6GBAHpO8TAYXSjaJc0eNWMbbYSRcT4XrfbCNUkYqISUeLnfCVeLcxq20u9U0On4fSDALDa7hPWbw+nW056BBjVbwwGDTEjsKNY65hmZigrA8nJ4ZDgsM2oPVGp0WXYQYz0epqS94V3RWDiGUH7etP3AMhwZr09nHkZGjzFisMHTsceD6HjTEy6cgel7cTBKM6knfNl6UgKkqOTz79cvEeH2fOnTTKZcQZjE1RVRYaqau6u323HcneLZNxsLtaLk8//uXh7vDcYhWgdPqW56WjJIiy228oh11DkIYn0OgazV8So8Wk+yxom7huse8EBv2BWhlktC+ZyLzBGfPNoWE5lMDzkGzDqiGQO5uPndxzMDl8NW+oivQz/XqFOLz/+Y0cwJUmGf28Lva4RGPitBt9MPdfppzqAkeHfe6DipnRcGRgZZrWdFEXMmVYBmNoiifXq08df/3smN2JAL9+GtLcEGEsaGBlmtQNJgNElNKVHhCTWqysBpsyIkeHfK0ERmPflgJFhVitNLzmY8OBgHjWSWC+jiEEHAyPDvydFL08+/3KYiJHh35On1y+vBJj9FvCkfFmsbC0j5hlJWtECnr8DGClmtQolwPwSPmgB76kjSRRFzJZr4VLMajXRlmvh3xOSWGIt/F0RGClmtRqqYGXze0USK3ctXIozq+Zai5hnJHMt1sLhGcmKoqZUNZL/B2Oyr0jRr0caAAAAAElFTkSuQmCC)

   ## 3.8.4 다변량 정규분포와의 관계

   $Let\;\;X=(X_{1},X_{2},\cdots,X_{p-1})$ 의 $p-1$개의 공변량과 반응 변수 $Y$를 고려하자.

   $(X,Y)\sim N_p$ 라고 가정하자.

   $Let\;\;f(X,\beta)=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_{p-1}X_{p-1}$ 라고 하자.

   그러면,

   $$
   Z=(Z^{(1)},Z^{(2)})\sim N_n(\mu,\Sigma) \longrightarrow Z^{(1)}_r\mid Z^{(2)}_{n-r}\sim N_r(\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(Z^{(2)}-\mu_2),\;\; \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
   $$

   임을 이용하여 다음을 얻는다.

   $$
   \begin{align*}f(X, \beta) & = E(Y | X) \\& = E(Y) + \text{Cov}(Y, X) \text{Cov}(X)^{-1} (X - E(X)) \\& = E(Y) - \text{Cov}(Y, X) \text{Cov}(X)^{-1} E(X) + \text{Cov}(Y, X) \text{Cov}(X)^{-1} X \\& \equiv \beta_0 + (\beta_1, \cdots, \beta_{p-1})^T X\end{align*}
   $$

   즉,

   - 회귀계수의 모집단ver 표현은

     $$
     \beta_0=E(Y)-Cov(Y,X)Cov(X)^{-1}E(X)
     $$

     $$
     (\beta_1,\cdots,\beta_{p-1})^T=Cov(X)^{-1}Cov(X,Y)
     $$

   - 단순 선형회귀 모형의 경우 회귀계수의 표본ver 표현은

$$
\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}
$$

$$
\hat{\beta}_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
$$
