---
title: "회귀 진단(Regression Diagnostics)"
date: 2023-09-23 00:00:00 +09:00
categories: regression
published: true
math: true
---

# Chapter 4. 회귀 진단(Regression Diagnostics)

## 4.1 소개

회귀진단(Model Checking)

1. **선형성**: $E(Y)=\sum^{p-1}_{j=0}\beta_jX_j$
   1. 위반시 → 비선형 회귀, 일반화 선형 모델, 비모수 회귀, **$\cdots$**
2. 오차항의 분포: $\epsilon_i \sim N(0,\sigma^2)$
   1. 위반시 → 더빗-왓슨 검정, 로버스트 회귀**, $\cdots$**
3. 다중공선성
   1. 위반시 → 릿지 회귀**, $\cdots$**
4. 영향력 관측치

## 4.2 잔차

$\beta$의 추정치로 $\hat{\beta}$를 사용하는 것 처럼 $\epsilon_i$의 실현치로 $e_i$를 생각하자. 하지만, 불행히도 오차의 독립성을 가정하고 구한 $e_i$들은 더 이상 독립이 아니고 상관을 가지고 있다.

1. 정의

$$
e_i=Y_i-\hat{Y}_i
$$

$$
e=y-\hat{y}
$$

1. 성질

$$
E(e)=0, \;\;\;Cov(e)=(I-H)\sigma^2
$$

$$
\begin{align*}\text{Var}(e_i) &= (1 - h_{ii})\sigma^2 \\\text{Cov}(e_i, e_j) & = -h_{ij}\sigma^2 \qquad (i \neq j)\end{align*}
$$

참고로 영향렬 행렬 $H$의 원소는 다음과 같다.

$$
h_{ij}=x^T_i(X^TX)^{-1}x_j
$$

$x_i = \begin{pmatrix} 1, & x_{i1}, & x_{i2}, & \dots, & x_{i,p-1} \end{pmatrix}^T$ 를 $X$의 $i$번째 행 벡터 라고하면 $e_i$와 $e_j$의 상관계수는 다음과 같다.

$$
\text{Corr}(e_i, e_j) = -\frac{h_{ij}}{\sqrt{1-h_{ii}}\sqrt{1-h_{jj}}} \qquad (i \neq j)
$$

### 4.2.1 내 표준화 잔차

잔차는 척도무관하지 않아서 표준화를 시켜야한다.

$$
{e_i-0\over \sigma \sqrt{1-h_{ii}}}
$$

하지만 $\sigma$가 unknown이기 때문에 $s={e^Te\over n-p}$로 대체해야한다. 그러면,

$$
r_i={e_i\over s \sqrt{1-h_{ii}}}
$$

이것을 내표준화 잔차라 부르고 이것은 $beta$분포를 따름이 알려져있다.

$$
\frac{r^2_i}{n-p} \sim \text{Beta}\left(\frac{1}{2}, \frac{n-p-1}{2}\right)
$$

### 4.2.2 외 표준화 잔차

외 표준화 잔차는 다음과 같다(교차검증, 잭나이프).

$$
r^*_i={e_i\over s_{(i)}\sqrt{1-h_{ii}}}\sim t(n-p-1)
$$

← $s_{(i)}:$ $i$번째 관측치를 제외하고 $n-1$개의 관측치에 기반한 $\sigma$의 불편 추정치

이것은 다음과 같음을 보일 수 있다.

$$
s^2_{(i)}=s^2\cdot {n-p-r^2_i\over n-p-1}
$$

그리고 다음도 성립한다.

$$
r^*_i = r_i \sqrt{\frac{n-p-1}{n-p-r^2_i}}
$$

## 4.3 레버리지 (예측값을 자신으로 끌어당기는 힘)

1. 레버리지의 정의

   $h_{ii}=x^T_i(X^TX)^{-1}x_i$ :$x_i$$i$번째 레버리지

   단순 선형회귀의 경우,

   $$
   h_{ii}=\left[
   \begin{matrix}
       1 & x_i \\
   \end{matrix}
   \right] {1\over S_{_{XX}}}\left[
   \begin{matrix}
        {\sum x^2_i\over n} & -\bar{x} \\ -\bar{x} & 1
   \end{matrix}
   \right] \left[
   \begin{matrix}
       1 \\ x_j
   \end{matrix}
   \right]={1\over n}+{(x_i-\bar{x})(x_j-\bar{x})\over S_{_{XX}}}
   $$

   $$
   h_{ii}={1\over n}+ {(x_i-\bar{x})^2\over S_{_{XX}}}
   $$

2. 레버리지의 성질

   $$
   \sum^n_{i=1}h_{ii}=p\;\;\;\;\;\;\;\cdots \;\;(1)
   $$

(증명)

$$
\sum_{i=1}^n h_{ii} = \text{tr}(H) = \text{tr}[X(X^T X)^{-1} X^T] = \text{tr}[(X^T X)^{-1} X^T X] = \text{tr}(I_p) = p
$$

$$
{1\over n}\le h_{ii} \le 1 \;\;\;\;\;\;\;\cdots \;\;(2)
$$

$$
h^2_{ij}\le h_{ii}h_{jj} \;\;\;\;\;\;\;\cdots \;\;(3)
$$

1. 높은 레버리지 포인트

   $h_{ii}$가 높은 것에 대응하는 $x_i$를 high-leverage point 라고 부른다.

   또한

   $$
   \hat{y}_i=x^T_i(X^TX)^{-1}X^Ty=\sum^n_{j=1}x^T_i(X^TX)^{-1}x_jy_j=h_{ii}y_i+\sum_{j \neq i}h_{ij}y_j
   $$

   즉, $\hat{y}_i$ 는 $y_i$ 에 영향을 많이 받지만, $h_{ii}$ 가 크면 $x_i$ 에 의해서도 영향을 많이 받는다.

   통상 ${2p\over n}$보다 크면 높은 레버리지로 판단한다.

## 4.4 영향력 관측치

1. 정의: $\hat{\beta},s^2$ 등 추정치에 큰 영향을 미치는 관측치를 영향력 관측치라 한다.

### Remarks

1. 어느 추정치에 영향을 미치는가?
2. 개별 영향력 관측치 vs 영향력 관측치 군
3. 가면효과 vs 늪 현상 ← 마치 길항작용(가면) vs 상승효과(늪)와 같다.

## 4.5 영향력 측도 (관측치의 영향력 측정하는 방법)

1. 소거법: $i$번째 관측치(observation)을 제거한 LSE추정치 $$\hat{\beta}_{(i)}$$에 대해 $\hat{\beta}-\hat{\beta}_{(i)}$을 계산하여 비교한다. ← 즉, 하나씩 빼보면서 확인한다.
2. **무한소교란법: $y_j\sim N(x^T_j\beta,\;\;{\sigma^2\over w_j})$ 을 가정하고, $w_j$가 $j=i$인 경우만 0~1사이의 값을 부여하고 나머지는 1의 값을 부여하므로써 $i$번째 관측치의 $\hat{\beta}$에 대한 영향력을 $1 \rightarrow0$으로 줄여가며 여러 각도에서 살펴보는 방법이다.**
3. **~~국소 영향력~~**
4. **~~대치법~~**

### 4.5.1 소거법에 의한 측도

1. Cook’s Distance (쿡의 거리)

$$
C_i = \frac{1}{p} (\hat{\beta} - \hat{\beta}_{(i)})^T \text{Cov}(\hat{\beta})^{-1} (\hat{\beta} - \hat{\beta}_{(i)})
$$

이 때, $(\hat{\beta}-\hat{\beta}_{(i)})$을 다음과 같이 쓸 수 있다.

$$
\hat{\beta}-\hat{\beta}_{(i)}={(X^TX)^{-1}x_ie_i\over (1-h_{ii})}
$$

따라서, $C_i$를 다시 써보면 다음과 같다.

$$
C_i={1\over p\sigma^2}\cdot {e^2_ih_{ii}\over (1-h_{ii})^2}
$$

즉, ‘$i$번째 데이터를 제거한다면 우리 회귀선은 얼마나 크게 바뀌게 될까?’에 대한 지표이다. 잔차와 지레값의 증가함수로 표현된다.

쿡 통계량의 95% 백분위수에 해당하는 기준이 적절하다고 밝혀졌으므로 아래의 기준값을 사용하자.

$$
C_i\ge{3.67 \over (n-p)}
$$

이제, 관측치군의 영향력을 위해, $K=\{i_1,\cdots,i_k  \}$를 $size\;k$인 집합의 index라고하자. 그러면,

$$\hat{\beta}-\hat{\beta}_{(K)}$$를 고려하면 $$\hat{\beta}_{(K)}=(X^T_{(K)}X_{(K)})^{-1}X^T_{(K)}y_{(K)}$$ 에 의해 쿡의 거리는 다음과 같다.

$$
C_K={1\over p}(\hat{\beta}-\hat{\beta}_{(K)})^TCov(\hat{\beta})^{-1}(\hat{\beta}-\hat{\beta}_{(K)})
$$

한편 $\hat{\beta}-\hat{\beta}_{(K)}$은 다음과 같이 표현 가능하므로

$$
\hat{\beta}-\hat{\beta}_{(K)}=(X^TX)^{-1}X^T_K(I-H_K)^{-1}e_K
$$

- $H_K=X_K(X^TX)^{-1}X^T_K$

따라서, 쿡의 거리는 다음과 같다.

$$
C_K = \frac{1}{p \sigma^2} e^T_K (I - H_K)^{-1} H_K (I - H_K)^{-1} e_K
$$

그 외

1. ~~Andrews-Pregibon Statistic~~
2. ~~DFBETAS~~
3. ~~DFFITS~~
4. ~~COVRATIO~~

## 4.6 이상치 검정(Test for Outlier)

$n$개의 관측치 중 $k$개의 관측치를 검정하기위해, 이상치 전이 모형(outlier shift model)을 고려하자.

$$
y=X\beta+Z\gamma+\epsilon
$$

$$
Z_{n\times k}=\left[ \begin{matrix}
0_{_{(n-k)\times k}}\\
I_k
\end{matrix} \right]
$$

$X$를 $n-k$개의 정상데이터와 $k$개의 outlier가 의심되는 데이터로 분리 시켜서$Z\gamma$를 얻을 수 있다.

이상치 존재에 대한 귀무가설은 $H_0:\gamma=0$ 으로 놓으면, 귀무가설 하에서 이상치 전이 모형은 축소된다.

$$
y=X\beta+\epsilon
$$

이제, 영향력행렬 $H$를 다음과 같이 분해해보자.

$$
H=\left[ \begin{matrix}
H_{11} & H_{12}\\ H_{21} & H_{22}
\end{matrix} \right]
$$

- $H_{11}:(n-k)\times (n-k)$
- $H_{12}:(n-k)\times k$
- $H_{21}:k\times (n-k)$
- $H_{22}:k \times k$

또한, 오차벡터 $e=(e^T_1,e^T_2)^T$ 도 쪼갠다.

검정통계량을 유도하기 위해 다음을 참고하자.

$$
\hat{\gamma}=(I_K-H_{22})^{-1}e_{_{2}}
$$

$$
\hat{\gamma}Z^T(I_n-H)y=\hat{\gamma}(0,I_k)e=\hat{\gamma}^Te_{_{2}}=e^T_{2}(I_k-H_{22})^{-1}e_{_{2}}
$$

따라서, 검정통계량은 다음과 같다.

$$
F = \frac{\frac{e^T_2 (I_k - H_{22})^{-1} e_2}{k}}{\frac{RSS - e^T_2 (I_k - H_{22})^{-1} e_2}{n-p-k}}
$$

special case로 $k=1$인 경우에는 (즉, $i$번째 관측치가 outlier인가) 다음처럼 축소된다.

$$
e^T_2 (I_k - H_{22})^{-1} e_2 = \frac{e^2_i}{1 - h_{ii}}
$$

$$
\frac{(n-p-1)e^2_i}{(1-h_{ii})\text{SSE} - e^2_i} \sim F(1, n-p-1)
$$

내 표준화 잔차가 다음과 같았던 점을 참고하라.

$$
r^2_i={e^2_i\over s^2(1-h_{ii})}
$$

그러면 $F$통계량은 $F_\alpha(1,k)=t_{\alpha/2}^2(k)$에 의해 다음과 같다.

$$
t^2_i = \frac{(n-p-1)r^2_i}{n-p-r^2_i} = \left(\frac{e_i}{s_{(i)}\sqrt{1-h_{ii}}}\right)^2
$$

이것은 다름아닌 외 표준화 잔차의 제곱이다.

## 4.7 다중공선성(II)

### Methods of Checking the Existence of Multicollinearity

1. VIF(variance inflation factor)

   $$
   VIF_k = \frac{1}{1 - R^2_k} \quad \text{for} \quad k = 1, 2, \dots, p-1
   $$

   - $\underset{K}{max}(VIF_K) >10$ 이면 다중공선성이 존재한다고 본다.
   - $$R^2_k={SSR_k\over SST_k}$$로서 $k$번째 설명변수 $X_k$를 반응변수로 놓고, 나머지 $X_{(k)}$들로 회귀분석시 얻어지는 $R^2$값이다. $$0 \le R^2_k\le 1$$이며 1에 가까울수록 나머지 $X_{(k)}$들과 $X_k$가 상관이 높다는 뜻이다.

2. $X^TX$의 고유값

   $X$의 특이값을 $d_1\ge d_2\ge \cdots \ge d_p \ge 0$ 이라 하자. 조건수는 다음과 같이 정의된다.

   $$
   \kappa(X) = \frac{d_1}{d_p} = \left(\frac{\lambda_1}{\lambda_p}\right)^{\frac{1}{2}}
   $$

   즉, 가장 큰 고유치와 가장 작은 고유치간의 격차가 커질수록 다중공선성일 가능성이 높아진다.

   - 통상 $\kappa(X) > 40$ 를 기준으로 한다고 한다.
   - $X^TX$ 또는 $W^TW$의 고유값을 조사해본다. 만약 선형종속이라면 $Rank(X)$가 줄어들 것이고 $Rank(X^TX)$도 $p$보다 작아지게 된다. 랭크는 곧 0아닌 고유값의 개수이므로 0에 가까운 $\lambda_i$가 있다면 다중공선성을 의심할 필요가 있다. $VIF$보다 $\lambda_i$를 이용하는 장점은 고유벡터에서 선형종속의 관계 형태를 대략적으로 알 수 있다는 것이 장점이다.

## 4.8 자기상관과 더빈-왓슨 검정(보류)

1. 자기상관 모형
2. 더빈-왓슨 검정
3. 변수변환 방법
