---
title: "선형 회귀모형 변형(Model Transformation)"
date: 2023-09-25 00:00:00 +09:00
categories: regression
published: true
math: true
---

# Chapter 6. 선형 회귀 모형 변형

# 6.1 더미 변수

## 6.1.1 더미 변수 1개인 경우

$$
Z =
\begin{cases}
1 & \text{if }\;newspaper \\
0 & \text{if }\;broadcast
\end{cases}
$$

$Z$라는 변수를 지시함수 처럼 사용하면 다음과 같은 모형을 고려할 수 있다.

$$
Y=\beta_0+\beta_1X+\beta_2Z+\epsilon
$$

따라서,

$$
Z=0:E(Y)=\beta_0+\beta_1X \\
$$

$$
Z=1:E(Y)=(\beta_0+\beta_2)+\beta_1X
$$

### Remark

만약 $C$개의 카테고리를 갖는 카테고리 변수가 있다면, 반드시 $C-1$개의 더미 변수를 사용해야 한다. 만약 4개의 범주를 갖는다고 하면 다음과 같이 3개의 더미 변수를 사용하자.

$$
Z_1 =
\begin{cases}
1 & \text{if }\;A \\
0 & \text{if }\;otherwise
\end{cases}
$$

$$
Z_2 =
\begin{cases}
1 & \text{if }\;B \\
0 & \text{if }\;otherwise
\end{cases}
$$

$$
Z_3 =
\begin{cases}
1 & \text{if }\;C \\
0 & \text{if }\;otherwise
\end{cases}
$$

$D$는 $Z_1=Z_2=Z_3=0$일 때 주어짐을 기억하라. 따라서 모델은 다음과 같다.

$$
Y=\beta_0+\beta_1Z_1+\beta_2Z_2+\beta_3Z_3+\epsilon
$$

## 6.1.2 더미 변수의 확장

1. 상호작용 항

$$
Y=\beta_0+\beta_1X+\beta_2Z+\beta_3XZ+\epsilon
$$

그러면 평균 반응은 다음과 같다.

$$
Z=0:E[Y]=\beta_0+\beta_1X
$$

$$
Z=1:E[Y]=(\beta_0+\beta_2)+(\beta_1+\beta_3)X
$$

기울기가 달라짐을 확인할 수 있다.

1. 변환점

   사전에 변환점을 알려져있다면,

   $$
   Z =
   \begin{cases}
   1 & \text{if }\;X>20 \\
   0 & \text{if }\;X\le20
   \end{cases}
   $$

   모델은 다음과 같이 설정할 수 있다.

   $$
   Y=\beta_0+\beta_1X+\beta_2(X-20)Z+\epsilon
   $$

   그러면 평균 반응은 다음과 같다.

   $$
   x\le20:E[Y]=\beta_0+\beta_1X
   $$

   $$
   x>20:E[Y]=(\beta_0-20\beta_2)+(\beta_1+\beta_2)X
   $$

## 6.1.3 회귀모형으로 ANOVA

일원 분산분석 모형을 고려해보자.

$$
Y_{ij}=\mu+\alpha_i+\epsilon_{ij} \qquad\begin{cases}
i=1,2,\cdots,c \\
j=1,2,\cdots,n_i
\end{cases}
$$

- $Y_{ij}:i$번째 처리에서 $j$번째 $obs$
- $\mu:$ 전체적으로 공통적인 효과
- $\alpha_i:$ $i$번째 처리의 상대적인 효과
- $\epsilon_{ij}:$ $i$번째 처리에서 $j$번째 $error$

예를들어, $c=4$이고 $n_i=3$인 경우를 보면 아래와 같다.

$$
\beta=(\mu,\alpha_1,\alpha_2,\alpha_3,\alpha_4)^T
$$

$$
X= \left[ \begin{matrix}
1&1&0&0&0\\
1&1&0&0&0\\
1&1&0&0&0\\
1&0&1&0&0\\
1&0&1&0&0\\
1&0&1&0&0\\
1&0&0&1&0\\
1&0&0&1&0\\
1&0&0&1&0\\
1&0&0&0&1\\
1&0&0&0&1\\
1&0&0&0&1\\
\end{matrix} \right]
$$

하지만, $X$는 $full-Rank$가 아니다. 이러한 점을 극복하려면 더미 변수를 사용해야 한다.

$$
Z_1 =
\begin{cases}
1 & \text{if }\;처리그룹1 \\
0 & \text{if }\;otherwise
\end{cases}
$$

$$
Z_2 =
\begin{cases}
1 & \text{if }\;처리그룹2 \\
0 & \text{if }\;otherwise
\end{cases}
$$

$$
Z_3 =
\begin{cases}
1 & \text{if }\;처리그룹3 \\
0 & \text{if }\;otherwise
\end{cases}
$$

그러면 모형은 다음과 같다.

$$
Y_i=\beta_0+\beta_1Z_{i1}+\beta_2Z_{i2}+\beta_3Z_{i3}+\epsilon_i \qquad i=1,2,\cdots,12
$$

$H_0:\alpha_1=\alpha_2=\alpha_3=\alpha_4=0$ 을 검정하는 것은 $H_0:\beta_1=\beta_2=\beta_3=0$을 검정하는 것과 동일한 것임을 인지하라(단, 해석에 주의 필요).

# 6.2 다항회귀(Polynomial Regression)

## 6.2.1 $k$차 다항회귀

1개의 공변량과 $k$차 다항회귀는 다음과 같다.

$$
Y=\beta_0+\beta_1X+\beta_2X^2+\cdots+\beta_kX^k+\epsilon
$$

1. 추정

다항회귀에서 $X$ 컬럼 벡터들 간에는 매우 큰 상관계가 있기 떄문에 공변량들을 변환 시켜야만 한다.

주로 직교 다항이 많이 쓰인다.

$$
Y_i=\alpha_0\phi_0(X_i)+\alpha_1\phi_1(X_i)+\cdots+\alpha_k\phi_k(X_i)+\epsilon_i
$$

$\phi_r(X)$는 $r$차 직교 다항이다($\sum^n_{i=1}\phi_r(X_i)\phi_s(X_i)=0 \qquad, \forall r\neq s$을 만족하는).

$$
\hat{\alpha}_r={\sum\phi_r(X_i)Y_i \over \sum \phi^2_r(X_i)}={\vec{\phi_r}\cdot \vec{Y}\over \vec{\phi_r}\cdot \vec{\phi_r} }\qquad ,r=0,1,\cdots,k
$$

1. 적절한 $k$차수 결정
   1. $Y=\beta_0+\beta_1X+\epsilon$을 적합하고 $H_0:\beta_1=0$ 검정을 한다. 만약 기각되지 않으면 중단하고($k=1$), 기각되면 STEP2 로 간다.
   2. $Y=\beta_0+\beta_1X+\beta_2X^2+\epsilon$을 적합하고 $H_0:\beta_2=0$을 검정한다. 만약 기각되지 않으면 중단하고($k=2$), 기각되면 STEP3로 가서 이와같이 반복한다.

## 6.2.2 반응 표면 분석(Response surface analysis)

### 목적

추정량의 유의성과 같은 분석보다는 반응표면 형태에 대한 분석과 반응 변수의 최적 조건을 찾는 것이 주 목적이다.

최적화: 시스템의 출력을 최대화하거나 최소화하기 위한 입력 변수의 최적 조합을 찾는다.

해석: 시스템이 어떻게 동작하는지 이해하기 위해 입력 변수와 출력 변수 사이의 관계를 모델링 한다.

### 구성요소

- 입력 변수(Factors)
- 반응 변수(Response)
- 반응 표면(Response surface): 주로 다항식으로 표현됨

### 장점

- 복잡한 시스템을 간단한 수학적 모델로 근사할 수 있어, 분석과 최적화가 용이하다.
- 너무 단순화되면 실제 시스템을 정확히 반영하지 못할 수 있다. 시간이 많이 소요된다.

### 분야

- 공학, 제조, 품질 관리 등

1. 2차 모형과 1개의 공변량 case

   $$
   \hat{Y}=\hat{\beta}_0+\hat{\beta}_1X+\hat{\beta}_2X^2
   $$

   모형이 위와같을 때, $Y$의 최적조건(optimal condition)은 1차 편미분으로 부터 얻을 수 있다.

   $$
   {d\hat{Y}\over dX}=\hat{\beta}_1+2\hat{\beta}_2X=0
   $$

   위의 해를 구하면 다음 극점을 얻는다.

   $$
   X_m=-{\hat{\beta}_1\over 2\hat{\beta}_2}
   $$

   그리고 극점에서 $\hat{Y}$의 값은 다음과 같다.

   $$
   \hat{Y}_m=\hat{\beta}_0-{\hat{\beta}^2_1\over 4\hat{\beta}_2}
   $$

   또한, 2차 편미분은 다음과 같다.

   $$
   {d^2\hat{Y}\over dX^2}=2\hat{\beta}_2
   $$

   따라서, $\hat{\beta}_2>0$이면 $\hat{Y}_m$은 최솟값이고, $\hat{\beta}_2<0$이면 $\hat{Y}_m$은 최댓값이다.

2. 1차 모형과 2개의 공변량 case

   $$
   \hat{Y}=\hat{\beta}_0+\hat{\beta}_1X_1+\hat{\beta}_2X_2
   $$

   위와 같은 모형을 갖고있다고 가정하자. $Y$의 최적점인 극점 $X_1과X_2$를 찾을 수 없다. 오직 증가(또는 감소)하는 방향만 알 수 있다. 따라서 $Y$를 고정시키고 다음을 얻는다.

   $$
   X_2=-{\hat{\beta}_1\over \hat{\beta}_2}X_1+c
   $$

   따라서, ${\hat{\beta}_1\over \hat{\beta}_2}$은 기울기에 수직인 방향이며, 최대 상승(감소)를 제공한다. 이를 경사 상승(하강)이라 한다.

3. 2차 모형과 $k$개의 공변량 case

   $$
   Y=\beta_0+\sum^k_{i=1}\beta_iX_i+\sum^k_{i=1}\beta_{ii}X_i^2+\sum^k_{i<j}\beta_{ij}X_iX_j+\epsilon
   $$

   위의 모형을 행렬 노테이션으로 나타내면 다음과 같다.

   $$
   Y=\beta_0+x^T\beta+x^T\mathrm{B} x^T+\epsilon
   $$

   $$
   x=(X_1,X_2,\cdots,X_k)^T\qquad\beta=(\beta_1,\beta_2,\cdots,\beta_k)^T\qquad

   \mathrm{B}= \left[ \begin{matrix}
   \beta_{11}&{\beta_{12}\over 2}&\cdots&{\beta_{1k}\over 2}\\
   \beta_{12}&{\beta_{22}}&\cdots&{\beta_{2k}\over 2}\\
   \vdots&\vdots&\ddots&\vdots\\
   {\beta_{1k}\over 2}&{\beta_{2k}\over 2}&\cdots&{\beta_{kk}}
   \end{matrix} \right]
   $$

   그리고 적합된 모형은 다음과 같다.

   $$
   \hat{Y}=\hat{\beta}_0+x^T\hat{\beta}+x^T\hat{\mathrm{B}}x
   $$

   1차 편미분은 다음과 같다.

   $$
   {\delta \hat{Y}\over \delta x}=\hat{\beta}+2\hat{\mathrm{B}}x=0
   $$

   극점은 다음과 같다.

   $$
   x_s=-{1\over2}\hat{\mathrm{B}}^{-1}\hat{\beta}
   $$

   극점에서의 반응분석은 정준 분석 또는 잠재적 관계 분석(canonical analysis)라 부른다. 고유값 분석과 컨투어 플롯이 주로 쓰인다. 이제, 극점에서의 적합된 값은 다음과 같다.

   $$
   \hat{Y}_s=\hat{\beta}_0+x^T_s\hat{\beta}+x^T_s\hat{\mathrm{B}}x_s
   $$

   $w=x-x_s$라고 하면 다음처럼 표현된다.

   $$
   \hat{Y}=\hat{Y}_s+w^T\hat{\mathrm{B}}w
   $$

   따라서, $\hat{\mathrm{B}}$가 양정치(positive definite) ← $\forall{\lambda>0}$ 이면 $\hat{Y}_s$는 최소값, $\hat{\mathrm{B}}$가 음정치 행렬(negative definite)이면 $\hat{Y}_s$는 최댓값이다.

   정리하자면

   1. $\forall \lambda_i>0$ 이면 $\hat{Y}_s$ 는 minimum

   2. $\forall \lambda_i<0$ 이면 $\hat{Y}_s$ 는 maximum

   3. 만약, $\lambda_i$ 중 음수와 양수가 섞여있다면 $\hat{Y}_s$또한 최소 또는 최대 이며, 이에 대응하는 $x_s$를 안장점이라 부른다.

      ![Untitled](https://miro.medium.com/v2/resize:fit:1400/1*zgEDCMaC2oo_TFwiBia7Vw.png)

## 6.3 가중최소제곱(WLS)

### 이분산성이 나타나는 경우

- 나이가 들수록 순자산이 달라지는 경향
- 회사 규모가 커질수록 매출이 엇갈리는 경향
- 유아의 키가 커질수록 체중이 분산되는 경향

검색하면 "가장 높은 변동성은 가장 작은 변동성의 4배를 초과해서는 안 됩니다"와 같은 다양한 경험 법칙을 찾을 수 있다. 문제의 규모를 통계적으로 결정하기 위한 [여러 가지 테스트](https://en.wikipedia.org/wiki/Heteroscedasticity#Detection) 도 있다.

---

지금까지 $Cov(\epsilon)=I\sigma^2$을 동질성 가정이라고 불리는 가정을 했다.

만약 이것이 위반된다면, 우리는 다음을 가정할 수 있다.

$$
Cov(\epsilon)=\sigma^2W^{-1}
$$

$W=diag(w_1,\cdots,w_n)$이고, 이 가정은 이분산성이라고 부른다. 따라서, $Var(\epsilon_i)={\sigma^2\over W_{ii}}$ 이다.

이제, 다음을 만족하는 가역행렬 $P$가 존재한다($P^T=P$인 대칭행렬이고 상수 취급).

$$
P^2=W
$$

오차의 선형변환으로 $\delta=P\epsilon$ 이라하자. 그러면 $E(\delta)=0$ 이고 공분산은 다음과 같다.

$$
Cov(\delta)=E(\delta \delta^T)=E(P\epsilon \epsilon^TP)=P^TCov(\epsilon)P=I\sigma^2
$$

이제, $y=X\beta+\epsilon$ 양변에 $P$를 곱하면

$$
Py=PX\beta+P\epsilon
$$

그리고 $Py=z, \qquad PX=Q$라고 하자. 그러면

$$
z=Q\beta+\delta
$$

따라서, 정규방정식은 다음과 같다.

$$
Q^TQ\hat{\beta}=Q^Ty
$$

동등하게 다음을 얻는다.

$$
X^TWX\hat{\beta}=X^TWy
$$

정리하면 $\hat{\beta}$는 다음과 같다.

$$
\hat{\beta}=(X^TWX)^{-1}X^TWy
$$

이 방법을 가중최소제곱 방법이라 부른다.

← 다르게 표현하면 $\epsilon \sim N(0,\Sigma)$ 이면 $\hat{\beta}=(X^T\Sigma^{-1}X)^{-1}(X^T\Sigma^{-1}y)$로 추정한다는 것

← 하지만 여전히 $GLM$에 이를 사용할 수 없다. solution: IRLS

## 6.4 Box-Cox 변환 모형

지금까지 반응변수는 정규분포를 따른다는 가정을 했다. 따라서, 이 정규성에 의심이 가는경우 Box and Cox는 다음을 제안했다. 다음을 만족하는 $\lambda$가 존재한다고($\lambda$ 는 근사적으로 정규성을 띈다).

$$
w_i = \begin{cases} \frac{y_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\\log y_i & \text{if } \lambda = 0\end{cases}
$$

행렬 노테이션은 다음과 같다.

$$
w(\lambda)=X\beta+\epsilon
$$

이것을 Box-Cox 변환 모형이라 부른다. $\beta와\sigma^2$에 더불어 $\lambda$도 추정해야한다. 그리고 이것들의 $MLEs$를 얻어야한다.

가능도 함수는 다음과 같다($W\sim N(X\beta,\;I\sigma^2)$).

$$
\begin{align*}L &= \left(2\pi\sigma^2\right)^{-\frac{n}{2}} \exp\left[-\frac{1}{2\sigma^2} (w - X\beta)^T (w - X\beta)\right] J(\lambda)\end{align*}
$$

자코비안 $J(\lambda)=\prod{\delta w_i\over \delta y_i}=\prod y_i^{\lambda-1}$은 $y \rightarrow w(\lambda)$로 변환하면서 발생하는 것이다.

프로파일링 방법으로 다음을 얻을 수 있다.

$$
\begin{align*}\hat{\beta}(\lambda) &= (X^T X)^{-1} X^T w(\lambda)\end{align*}
$$

$$
\begin{align*}s^2(\lambda) &= \frac{w(\lambda)^T (I - H) w(\lambda)}{n}\end{align*}
$$

이제 가능도에 위의 두 추정치를 대체하여 $\lambda$만 남긴 가능도를 얻을 수 있다.

$$
\begin{align*}l(\lambda : \hat{\beta}(\lambda), s^2(\lambda)) &\simeq -\frac{n}{2} \log s^2(\lambda) + (\lambda - 1) \sum \log y_i\end{align*}
$$

이제 위의 가능도를 0으로 만드는 해를 구하면 되지만 비선형이기 때문에 명확한 해를 구할 수 없다. 이 경우 수치적인 방법으로 해결할 수 있다(뉴튼-랩슨도 미분값이 깔끔하게 나오지 않아 쓰기 어려움).

→ 위 결과로 $\hat{\lambda}\approx{1\over2}$이면 $\sqrt{y}$변환, $\hat{\lambda}\approx 0$이면 $log\;y$변환, $\hat{\lambda}\approx1$이면 변환없이 사용하면 된다.

## 6.5 로버스트 회귀(Robust Regression)

오차항이 이중 지수분포(라플라스 분포) 처럼 꼬리가 두꺼운 정규분포를 따르는 예를 들어보자.

$$
\begin{align*}f(\epsilon) &= \frac{1}{2\sigma} e^{-\frac{|\epsilon|}{\sigma}} \quad , \; -\infty < \epsilon < \infty\end{align*}
$$

또한 가능도 함수는 다음과 같이 주어진다.

$$
\begin{align*}L &= \frac{1}{(2\sigma)^n} \exp\left[-\frac{\sum |y_i - x^T_i \beta|}{\sigma}\right]\end{align*}
$$

이제 $\beta$를 추정하기위해 가능도를 최대화하는 것은 $L_1-norm$ 회귀라 불리는 ${\sum\vert y_i-x^T_i\beta\vert}$를 최소화하는 것과 동치다. 정규분포에서 ${\sum( y_i-x^T_i\beta)^2}$ 를 최소화하는 것은 $L_2-norm$ 회귀였다.

일반적으로, $L_p-norm$ 회귀는 ${\sum( y_i-x^T_i\beta)^p}$를 최소화 한다($1\le p\le2$).

정리하면, 오차항이 이중지수분포를 따른다면 MLE는 $L_1$ 회귀가 됨을 보았다.

### 6.5.1 $M-Estimation$ (aka robust loss function in ML)

어떤 함수 $\rho()$를 위해, 만약 다음과 같은 수식으로부터 $\beta$를 얻었다면,

$$
\begin{align*}\min \sum_{i=1}^n \rho(\epsilon_i) &= \min \sum_{i=1}^n \rho(y_i - x^T_i \beta)\end{align*}
$$

$\rho()$를 $M-Estimator$라 부른다. 만약 $\rho(u)={1\over 2}u^2$ 이면, 이것은 $L_2-norm$회귀가 된다.

또한, $\rho(u)=\vert u \vert$이면 $L_1-norm$회귀가 된다.

이제 $\beta$의 로버스트 추정기($robust \;esimator$)를 얻기위해 우리는 다음을 고려하자.

$$
\begin{align*}\min \sum_{i=1}^n \rho\left(\frac{\epsilon_i}{s}\right) &= \min \sum_{i=1}^n \rho\left(\frac{y_i - x_i^T \beta}{s}\right)\end{align*}
$$

$s$는 $\sigma$의 $robust-estimator$이다. 이것은 다음과 같다.

$$
\begin{align*}s &= \frac{\text{median} \, |e_i - \text{median}(e_i)|}{0.6745}\end{align*}
$$

위처럼 하는 것이 좋다는 연구결과로부터 결정된 사실이다.

우리는 $n$이 크면 $s$는 $\sigma$의 $UE$이고 오차항들은 정규분포를 따른다는 사실을 보일 수 있었다.

최소값을 얻기위해 $\rho$를 $\beta_j$에 대해서 1차 편미분으로 부터 다음을 얻는다.

$$
\begin{align*}\frac{\delta \rho}{\delta \beta_j} &= \sum_{i=1}^n x_{ij} \, \rho'\left(\frac{y_i - x_i^T \beta}{s}\right) = 0\end{align*}
$$

$x_{ij}$는 $x_i$의 $j$번째 원소이다. $\rho^{'}$는 $\beta$의 비선형이기 때문에 명확한 해를 구할 수 없다.

대신 IRLS(Iterative Reweighted Least Squares)의 반복법을 통해 구한다.

행렬 노테이션으로 표현하면 다음과 같다.

$$
\begin{align*}X^T W_{\beta} X \beta &= X^T W_{\beta} y\end{align*}
$$

$W_{\beta}=diag(w_{1\beta}, w_{2\beta},\cdots,w_{n\beta})$ 인데 $\beta$는 모르는 값이기 때문에 $\hat{\beta}_0$을 초기값으로 사용하여 다음과 같은 과정을 반복한다.

$$
\begin{align*}\hat{\beta}_{c+1} &= X^T W_{\hat{\beta}_c} X \beta = X^T W_{\hat{\beta}_c} y \qquad \text{for } c = 0, 1, 2, \dots\end{align*}
$$

← 로버스트 회귀의 장점은 최소제곱법으로 여러번의 잔차 분석을 통해서 이상치인 $obs(1,3,4,21)$을 제거할 것을 자동으로 $obs(1,3,4,21)$들의 영향력을 줄여준다는 장점이있다.

← 따라서, 이상치가 의심되고 두꺼운 꼬리의 정규분포가 의심이 되면 후버나 햄팰을 사용하여 이상치에 $Robust$하게 만든다(이런 장점 때문에 최소제곱은 잘 안쓴다).

![Untitled](https://datamonje.com/wp-content/uploads/2021/11/Huber-loss-and-outliers.png)

### $Estimator$

### 최소제곱법(LSE)

$$
\begin{align*}\rho(\epsilon) &= \frac{1}{2}\epsilon^2 \\[1em]\dot{\rho}(\epsilon) &= \epsilon\\[1em]\ddot{\rho}(\epsilon) &= 1\end{align*}
$$

### 후버(Huber)

$$
\begin{align*}\rho(\epsilon) &= \begin{cases} \frac{1}{2}\epsilon^2 & \text{if } |\epsilon| \le 2 \\2|\epsilon| - 2 & \text{if } |\epsilon| > 2\end{cases}\\[1em]\dot{\rho}(\epsilon) &= \begin{cases} \epsilon & \text{if } |\epsilon| \le 2 \\2 \cdot \text{sign}(\epsilon) & \text{if } |\epsilon| > 2\end{cases}\\[1em]\ddot{\rho}(\epsilon) &= \begin{cases} 1 & \text{if } |\epsilon| \le 2 \\\frac{2}{|\epsilon|} & \text{if } |\epsilon| > 2\end{cases}\end{align*}
$$

- 제안된 조절상수: $a=2.0$

### 햄팰(Hampel)

$$
\begin{align*}\rho(\epsilon) &= \begin{cases} \frac{1}{2}\epsilon^2 & \text{if } |\epsilon| \le a \\a|\epsilon| - \frac{1}{2}a^2 & \text{if } a < |\epsilon| \le b \\\frac{a(c|\epsilon|-\frac{1}{2}\epsilon^2)}{c-b} & \text{if } b < |\epsilon| \le c \\a(b+c-a) & \text{if } c < |\epsilon| \end{cases}\\\dot{\rho}(\epsilon) &= \begin{cases} \epsilon & \text{if } |\epsilon| \le a \\a\text{ sign}(\epsilon) & \text{if } a < |\epsilon| \le b \\\frac{a(c\text{ sign}(\epsilon)-\epsilon)}{c-b} & \text{if } b < |\epsilon| \le c \\0 & \text{if } c < |\epsilon|\end{cases}\\\ddot{\rho}(\epsilon) &= \begin{cases} 1 & \text{if } |\epsilon| \le a \\\frac{a}{|\epsilon|} & \text{if } a < |\epsilon| \le b \\\frac{a(c|\epsilon|-1)}{c-b} & \text{if } b < |\epsilon| \le c \\0 & \text{if } c < |\epsilon|\end{cases}\end{align*}
$$

- 제안된 조절상수: $a=1.7 \qquad b=3.4 \qquad c=8.5$

![untitled](https://www.researchgate.net/publication/353399197/figure/fig1/AS:1048574771093505@1627011115922/Different-loss-functions-solid-lines-in-the-top-row-and-their-corresponding-derivatives.ppm)

### 6.5.2 영향력 함수(Influence function)

우리는 자료집단에서 다른 관찰값에 비해 유난히 작거나 큰 값으로 보통의 관찰값과는 다른 관
찰값을 **이상치**(Outlier)라 정의한다. 자료분석시 이상치의 발견이 중요한 것은 이상치가 평균과 분
산과 같은 통계량들에 많은 영향을 미치고, 자칫하면 이러한 통계량의 오용을 가져오기 때문이다.
이러한 이유로 이상치를 구분하고 판별하는 데 많은 연구가 진행되고 있다.
**영향함수**(Influence function)는 **이상치 발견과 선택에 쓰이는 방법중 하나**로 Hampel(1974)에
의해 처음으로 소개되었다. Hampel에 의하여 제안된 영향함수는 모수 및 거의 모든 통계량에 적
용 가능함을 보였고, Campbell(1978)은 판별분석(Discriminant analysis)에서 이상치(Outlier) 탐지에
영향함수를 이용하였고, Radhakrishnan and Kshirsagar(1981)은 다변량 분석에서 여러 가지 모수
에 대한 이론적인 영향함수들을 유도했다. 또한 Cook(1977), Cook and Weisberg(1980, 1982)는 회귀
분석에서 회귀진단방법으로, Critchley(1985)는 주성분 분석에서 영향력 있는 관찰값을 찾아내기 위
해 이 방법을 적용하였다. Kim(1992)은 이차원 분할표의 대응분석에서 얻어진 고유치들에 대한 영
향함수를 유도하였으며 이를 다차원 분할표의 대응 분석으로 확장하였고, Kim and Lee(1996),
Kim(1998) 등은 $\chi^2$통계량에 대한 영향함수들을 다루고 있다.

$T$는 분포함수에 대해 실수값을 갖는 범함수($real-valued\;functional$), 즉 일련의 모수이고, $F(t)$는 분포함수라고 하자. 그리고 $\delta_x(t)$는 점$x$에서 확률이 1인 CDF이다.

분포함수 $F$에 임의의 관찰값 $x$를 추가함으로써 생기는 혼합분포함수 $F_\epsilon$은 다음과 같다.

$$
\begin{align*}F_\epsilon &= (1-\epsilon)F + \epsilon\delta_x \qquad 0 < \epsilon < 1\end{align*}
$$

이때 $F_\epsilon$을 $F$의 섭동(perturbation)이라 한다.

Hampel은 범함수$T(F)$에 대한 $x$의 영향력 함수를 다음과 같이 정의했다.

$$
\begin{align*}\text{IF}(T,x) &= \left. \frac{\delta T(F_{\epsilon})}{\delta \epsilon} \right|_{\epsilon=0}\end{align*}
$$

예를 들어 모집단의 평균과 분산은 범함수 $T(F)$의 일종이므로 각각을 범함수

$$
\begin{align*}\mu(F) &= \mu = \int t \, dF\end{align*}
$$

$$
\begin{align*}\sigma^2(F) &= \sigma^2 = \int (t - \mu)^2 \, dF\end{align*}
$$

로 표현하면, 평균과 분산에 대한 영향력 함수는 정의로부터 다음과 같이 유도할 수 있다.

$$
\begin{align}\text{IF}(\mu,x) &= x - \mu \tag{2.3}\end{align}
$$

$$
\begin{align}\text{IF}(\sigma^2,x) &= (x - \mu)^2 - \sigma^2 \tag{2.4}\end{align}
$$

- (2.3)을 보면 $x$가 평균에 미치는 영향은 평균과의 거리로써 평균보다 작으면 (-)의 방향으로, 크면 (+)방향으로 영향을 준다는 것을 알 수 있다.
- (2.4)를 보면 $x$의 편차제곱$(x-\mu)^2$과 원래 분산 $\sigma^2$의 차이가 클수록 그 관찰값이 분산에 주는 영향이 크다는 것을 알 수 있다.

### **중요성**

1. **로버스트 통계**: Influence Function은 추정량이 이상치에 얼마나 민감한지 평가
2. **효율성 평가**
3. **신뢰구간**

## 6.6 역 회귀(Inverse Regression)

지금까지와 반대로 $y_0$가 주어지면 $x_0$를 예측도 가능할까?

우리는 이것을 역 회귀(or Calibration, Discrimination)이라고 부른다.

다음과 같은 단순 선형 회귀 모형을 생각해보자.

$$
Y=\beta_0+\beta_1X+\epsilon
$$

$y_0$가 given일 때 $x_0$의 추정은 다음과 같이 얻을 수 있다.

$$
\begin{align*}\hat{x}_0 = \frac{y_0 - \hat{\beta}_0}{\hat{\beta}_1}\end{align*}
$$

또한, $x_0$에 대한 신뢰구간은 다음을 풀어서 얻을 수 있다($t^2$은 $1-\alpha$ 경계를 의미).

$$
\begin{align*}\frac{(y_0-\hat{y})-E(y_0-\hat{y})}{\text{Var}(y_0-\hat{y})} &= \frac{(y_0-\hat{\beta}_0-\hat{\beta}_1x)^2}{s^2A^2} \le t^2\end{align*}
$$

- $A^2 = 1 + \frac{1}{n} + \frac{(x-\bar{x})^2}{\sum(x_i-\bar{x})^2}     = \frac{\text{Var}(y-\hat{y})}{s^2}$

위 식에서 $d=x-\bar{x}$ 라고 하면, 다음과 같은 부등식을 얻을 수 있다.

$$
\begin{align*}d^2\left(\hat{\beta}^2_1 - \frac{t^2 s^2}{\sum(x_i-\bar{x})^2}\right) - 2d\hat{\beta}_1(y_0-\bar{y}) + \left((y_0-\bar{y})^2 - t^2 s^2 \left(1 + \frac{1}{n}\right)\right) &\le 0\end{align*}
$$

이차 부등식이므로 $d_1,d_2$를 $d$의 해라고 해보자. 우리는 $d_1+\bar{X}\le X_0\le d_2+\bar{X}$ 이라는 범위를 얻을 수 있다.

한편, 우리가 원하는 이차함수의 유의미한 해($d_1,d_2$)를 갖을 조건은 $판별식>0$ 이 필요조건이다.

이차 부등식 $a_2x^2+a_1x+a_0<0$ 의 해가 유한한 구간이려면 $a_2>0$이어야 하는데, 이에 해당하는 식이 $\hat{\beta}_1^2>{t^2s^2\over \sum(x_i-\bar{x})}$ 이다. 이것은 $H_0:\beta_1=0$ 이다를 기각하는 조건과 동치이다.

즉, $\beta_1 \neq 0$인 조건하에서만 역 회귀를 할 수 있다는 뜻이다(=X와 Y간에 회귀관계가 있어야 한다).

$$
\begin{align*}\text{역 회귀가 가능하다} \longleftrightarrow X\text{와} Y\text{간 } H_0:\beta_1=0 \text{이 기각되어야 한다}\end{align*}
$$
