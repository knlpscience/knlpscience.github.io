---
title: "변수 선택(Feature Selection)"
date: 2023-09-25 00:00:00 +09:00
categories: regression
published: true
math: true
---

# Chapter 5. 변수 선택

## 5.3 변수 선택 방법

1. 모든 가능한 회귀
2. 전진 선택 (변수의 갯수가 많지 않을 때 안정적)
3. 후진 제거 (계산량은 적지만 한번 제거되면 다시 고려되지 않아 바람직하지 않음)

### 5.3.1 모든 가능한 회귀

최대 모형이 $p-1$개의 공변량을 갖고있다면 가능한 조합의 수는 $2^{p-1}$개 이다.

가장 확실하지만 $p$가 크면 계산량이 많은 것이 단점

### 5.3.2 전진 선택법 (Forward stepwise regression)

**알고리즘**

1. 현재 모델이 1개의 공변량을 갖고있다고 가정하고 F-test를 진행

   $$
   F^*_i={MSR_i \over S^2_i}
   $$

   그리고 $$\underset{1\le i \le k-1}{\mathrm{max}}F^*_i> F_\alpha(1,\;n-2)$$ 이면 해당 변수를 추가시킨다. 그렇지 않으면 기존 모델을 채택한다.

2. $X_7$이 위 과정에서 선택되었다고 가정하자. 또 다른 새로운 변수를 추가해서 부분 F-test(Partial F-test)를 진행한다.

   $$
   F^*_{i} = \frac{\text{MSR}(X_i \mid X_7)}{s^2(X_7, X_i)} = \left( \frac{\hat{\beta}_i}{\text{SE}(\hat{\beta}_i)} \right)^2 \quad \text{where} \quad i \neq 7
   $$

   그리고 $$\underset{i \neq 7}{\mathrm{max}}F^*_i>F_\alpha(1,\;n-3)$$이면, 새로운 변수는 추가되어 STEP 3으로 넘어가고그렇지 않으면 최종모형은 $X_7$에서 종료된다.

3. $X_3$이 위 과정에서 선택되었다고 가정하자. $X_3$이 주어졌을 때 $X_7$에 대한 부분 F-test를 진행한다(즉, $X_7$이 먼저 들어왔지만 반대로 $X_3$이 먼저 들어왔다 치면 어떻게 될까? 이다).

   $$
   F^*_{7} = \frac{\text{MSR}(X_7 \mid X_3)}{s^2(X_3, X_7)}
   $$

   그리고 $F_\alpha(1,\;n-3)$ 보다 크면 $X_7$은 살고 그렇지 않으면 모형에서 $X_7$은 제거된다.

4. $X_7$이 3에서 살아남았다면 2와 3을 반복한다.

## 5.4 모델 평가(Model Validation)

### 5.4.1 교차검증 (Cross validation)

1. 정의

   $\{1,\cdots,n\}$을 $K$개의 집합$I_1,\cdots,I_K$으로 분할한다. 그리고 함수를 정의한다. $\kappa(i)=j$인 $\kappa:\{1,\cdots,n\} \rightarrow \{1,\cdots, K\}$. $\{(X_1,Y_1),\cdots, (X_n,Y_n)\}$ 인 데이터셋에 대하여 $\hat{f}_{-j}$를 $I_j$에 해당하는 관측치들을 제외한 데이터로 추정한 추정치라고하자. 그러면 예측오차의 추정은 다음과 같다.

   $$
   \hat{PE}_{\text{CV}} = \frac{1}{n} \sum_{i=1}^{n} L\left(Y_i, \hat{f}_{-\kappa(i)}(X_i)\right)
   $$

   즉, $i$번째 데이터를 제외한 데이터셋으로 모델을 훈련시키고, $i$번째 데이터로 성능을 측정을 n개에 대해서 반복하여 평균낸다.

2. k-fold CV

   위의 과정을 확장한 것으로 $i$번째 데이터 셋을 제외한 데이터셋으로 모델을 훈련시키고, $i$번째 데이터 셋으로 성능을 측정한 것을 반복하여 평균낸다.

## 5.5 변수선택에서 진단

새로운 변수가 추가되면 $R^2$은 항상 증가한다.

: 기본적으로 모든 변수 차원 $n$에서 알고있는 변수의 갯수 $p$로 차원 축소가 된 것이기 때문에 $n-p$만큼의 차원에서 정보 손실이 발생한다. 이 손실된 양이 SSE이며 나머지가 SSR이다. 둘은 트레이드오프 관계가 있는데 따라서, 아주 조금의 설명력을 갖는 설명변수가 추가된다고 할지라도 SSR은 늘고, SSE는 줄어든다.

이런 단점을 보완하기 위해 몇 가지 지표가 제시되었다.

1. $R^2_{adj}$

$$
R^2_{\text{adj}} = 1 - \frac{\frac{SSE_p}{n-p}}{\frac{SST}{n-1}} = 1 - \frac{S^2_p}{\frac{SST}{n-1}}
$$

따라서, $R^2_{adj}$를 최대화 하는것은 $S^2_p(=MSE_p)$를 최소화하는 문제와 같다.

1. 멜로우즈 $C_p$

MSE는 언제나 편향과 분산의 합으로 나타낼 수 있다.

$$
E\left( (\mu_i - \hat{y}_i)^2 \right) = \text{Var}(\hat{y}_i) + \left[ E(\hat{y}_i) - \mu_i \right]^2
$$

여기서 $\Gamma_p$라는 것을 정의하자.

$$
\begin{align*}\Gamma_p &= \frac{MSE_p}{\sigma^2} \\&= \frac{1}{\sigma^2} \left[ \text{tr}\left(\text{Cov}(\hat{y})\right) + \left(E(\hat{y}) - \mu\right)^T \left(E(\hat{y}) - \mu\right) \right] \\&= p + \frac{1}{\sigma^2} \mu^T (I-H) \mu\end{align*}
$$

위 식은 미지의 모수를 포함하고 있으므로 직접 사용할 수는 없다.

$\Gamma_p$의 추정치를 위해 우선 $E(SSE_p)$를 구해보면 다음과 같음을 알 수 있다.

$$
\begin{align*}E(SSE_p) &= E(e^T e) \\&= E\left( y^T (I-H) y \right) \\&= \mu^T (I-H) \mu + (n-p) \sigma^2\end{align*}
$$

따라서, 다음이 성립한다.

$$
\Gamma_p={E(SSE_p)\over \sigma^2}-(n-2p)
$$

여기서 ${E(SSE_p)\over \sigma^2}$ 대신 ${SSE_p\over s^2}$을 사용하여 최종적으로 $\Gamma_p$의 추정치로 $C_p$를 쓴다.

$$
\hat{\Gamma}_p=C_p={SSE_p\over s^2}-(n-2p)
$$

- $${SSE_p\over s^2}$$:GOF
- $$(n-2p)$$:Penalty

즉, 적절한 $p$를 선택하는 것이 $C_p$를 최소화하는 것이다.

한편, $e^c$와 $e$를 각각 현재모형에서 잔차벡터, 완전모형에서 잔차벡터라고 하자. 만약 $q-1$이 완전모형에서 공변량 갯수이면, 멜로우즈 $C_p$는 다음과 같다.

{% raw %}

$$
\begin{align*}
C_p &= \frac{{e^c}^T {{e}^c}}{e^T e / (n-q)} - (n - 2p)
\end{align*}
$$

{% endraw %}

이제 $k$개를 제거하고 $n-k$개의 관측치에 기반한 $C_p$를 구하면,

$$
\begin{align*}C_{p(K)} &= \frac{SSE_{p(k)}}{s^2_{(k)}} - (n - k - 2p)\end{align*}
$$

그리고 다음과 같은 식으로 표현할 수 있음을 보일 수 있다.

$$
\begin{align*}C_{p(K)} &= \frac{(n-k-q) \left[ C_p + (n-2p) - {e^{c}}^T_K (I-H^c_K)^{-1} {e^{c}}_K / s^2 \right]}{(n-q) - e_K (I-H_K)^{-1} e_K / s^2} - (n - k - 2p)\end{align*}
$$

## 5.6 모델선택에서 참고사항

1. 쿨백라이블러 발산

   두 개의 $pdf$ $f$와 $g$가 주어졌다고 가정해보자. 쿨백라이블러 발산은 다음처럼 정의된다.

   $$
   \begin{align*}E_f\left[ \log\left(\frac{f(X)}{g(X)}\right) \right] &= \int \log\left(\frac{f(X)}{g(X)}\right) f(X) \, dx\end{align*}
   $$

   - 주의할 점은 $f$에서 평가되므로 상대적인 거리이지 절대적인 거리로 해석하지 말자.
   - 분포모양이 비슷할수록 $\rightarrow0$이 될것이고, 매우 다를수록 $\rightarrow \infty$일 것이다.
   - $KL(f\Vert g)=CE[f,g]-CE[f,f]$ 으로 표현할 수도 있다.
   - $KL(f\Vert g)\neq KL(g\Vert f)$

   ### 5.6.2 AIC (Akaike information criterion)

   $f(x:\theta_0)$와 $f(x:\theta)$을 각각 참인 모수하에서 얻은 $pdf$와 적합된 $pdf$라고 하자. 그러면

   $$
   \begin{align*}K(\theta) &\equiv -2 E_{\theta_0} \left[ \log f(X : \theta) \right] \\&= -2 \int \log f(x : \theta) f(x : \theta_0) \, dx\end{align*}
   $$

   그리고 다음을 고려해보자.

   $$
   {1\over 2}K(\theta)+\int \{logf(x)\}f(x)\;dx
   $$

   그리고 $g(x)\equiv f(x:\theta)$라고 하면 쿨백라이블러에 의해 다음을 얻는다.

   $$
   \begin{align*}\frac{1}{2} K(\theta) + \int \log f(x) f(x) \, dx &= -\int \log f(x : \theta) f(x) \, dx + \int \log f(x) f(x) \, dx \\&= \int \log\left(\frac{f(x)}{g(x)}\right) f(x) \, dx \\&= E_f\left[ \log\left(\frac{f(x)}{g(x)}\right) \right]\end{align*}
   $$

   즉, 이거는 $KL(f\Vert g)$이다. 작을수록 참(true)모형에 가깝다는 것이다.

   다른말로 $K(\theta)$를 최소화하면 되는 것이다. 하지만, 이것은 unknown parameter $\theta_0$를 포함하고 있기에 $K(\theta)$의 추정치를 사용해야한다.

   따라서, sample version인 $K_n(\theta)$를 고려해보자

   $$
   \begin{align*}K_n(\theta) &= -\frac{2}{n} \sum_{i=1}^{n} \log f(X_i : \theta)\end{align*}
   $$

   $\dot{K}_n(\theta)과 \ddot{K}_n(\theta)$를 $\theta$에 대한 1차 미분, 2차 미분 도함수라고 하자. 그리고 $\hat{\theta}=\hat{\theta}^{mle}$ 에 대하여 $\dot{K}_n(\hat{\theta})=0$이다. 그리고 다음을 정의하자.

   $$
   \begin{align*}K_n(\hat{\theta}) &= -\frac{2}{n} \sum_{i=1}^{n} \log f(X_i : \hat{\theta})\end{align*}
   $$

   마침내, $AIC$는 다음과 같이 정의된다.

   $$
   \begin{align*}AIC &= K_n(\hat{\theta}) + R_n \\&= -\frac{2}{n} \sum_{i=1}^{n} \log f(X_i : \hat{\theta}) + \frac{2p}{n} \\&= \frac{SSE_p}{n \sigma^2} + \frac{2p}{n}\end{align*}
   $$

   - 선형인 경우 $AIC=C_p$

   한편, 표본 $n$이 클수록 $(n\rightarrow \infty)$ $AIC$보다 더 큰 패널티를 주는 $BIC$는 다음과 같다.

   $$
   \begin{align*}BIC &= \frac{SSE_p}{n \sigma^2} + \frac{p \cdot \log(n)}{n}\end{align*}
   $$

   - $AIC$와 $GOF$항은 같다.
   - $n$이 클수록 패널티를 더 받는다.
   - $C_p,AIC,BIC$모두 $p$가 크면 패널티를 더 크게 받는 구조이다.

ref.
